{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bài toán\n",
    "\n",
    "Các khó khăn gặp phải khi xây dựng mô hình phân loại văn bản tiếng Việt:\n",
    "\n",
    "1. Việc tách từ tiếng Việt khá là phức tạp do có nhiều từ và cụm từ ghép lại với nhau, nhiều từ không có trong từ điển.\n",
    "2. Việc tìm được bộ từ điển tương đối đầy đủ khá khó khăn do tiếng Việt có nhiều từ ngữ cả trong lĩnh vực chuyên ngành và nhiều từ ngữ không có trong từ điển. Các từ mới, từ mượn được sinh ra hàng năm và chưa được cập nhật vào từ điển. Hiện tại mới chỉ tìm được một số ít từ phổ biến có trong các từ điển mở.\n",
    "3. Việc triển khai mô hình phân loại văn bản tiếng Việt khá là khó khăn do không có nhiều tài liệu tham khảo, đặc biệt là các tài liệu về xử lý ngôn ngữ tự nhiên tiếng Việt.\n",
    "4. Việc cài đặt môi trường lập trình cho Tensorflow cũng như pytorch gặp nhiều khó khăn khi cài đặt trên Windows, đặc biệt là khi sử dụng GPU.\n",
    "5. Việc chuẩn hoá văn bản tiếng Việt gặp rất nhiều thách thức về:\n",
    "    - Cách mã hoá văn bản Unicode\n",
    "    - Cách bỏ dấu tiếng Việt\n",
    "    - Xử lý sai chính tả\n",
    "    - Xử lý từ viết tắt, tiếng địa phương, ...\n",
    "    - Xử lý các từ ngữ không có trong từ điển\n",
    "7. Việc tách từ cũng gặp rất nhiều bất lợi\n",
    "    - Từ điển càng lớn thì việc tách từ càng chậm, đã thử nghiệm và tách từ mất 30p cho tập train.\n",
    "    - Tokenization phải đào tạo mô hình để thực hiện mới đem lại hiệu quả tốt. Đã thử nghiệm với các thư viện như pyvi, underthesea, vncorenlp, ... nhưng kết quả không tốt. Chậm quá\n",
    "    - Các thư viện tách từ tiếng Việt hiện tại chưa hỗ trợ tách từ cho các từ ngữ không có trong từ điển, các từ viết tắt, tiếng địa phương, ...\n",
    "    - Đã phải tự xây dựng thuật toán tách từ và mang lại hiệu quả tạm chấp nhận được.\n",
    "\n",
    "6. Hiện chưa có bộ StopWords tiếng Việt tương đối đầy đủ, nên việc xử lý StopWords cũng gặp nhiều khó khăn do đa phần là tự tạo không theo một tiêu chuẩn nào cả.\n",
    "    - Đã tự viết chương trình thống kê các từ xuất hiện trong tập train và tập test để tạo bộ StopWords tuy nhiên vẫn còn thiếu sót và chưa đầy đủ.\n",
    "    - Phải tìm đủ các từ đồng nghĩa để đưa vào bộ StopWords.\n",
    "\n",
    "- Input: một đoạn văn bản\n",
    "- Output: một trong các nhãn sau: Văn hoá, Kinh doanh, Thế giới, Thể thao, Pháp luật, Sức khoẻ, Đời sống, Chính trị xã hội, Khoa học, Vi tính"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thu thập dữ liệu"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dữ liệu được lấy dừ https://github.com/duyvuleo/VNTC/tree/master/Data/10Topics/Ver1.1 với thông tin như sau:\n",
    "\n",
    "**_Train_**\n",
    "\n",
    "| Topic            | Topic ID | #files |\n",
    "| ---------------- | -------- | ------ |\n",
    "| Chinh tri Xa hoi | XH       | 5219   |\n",
    "| Doi song         | DS       | 3159   |\n",
    "| Khoa hoc         | KH       | 1820   |\n",
    "| Kinh doanh       | KD       | 2552   |\n",
    "| Phap luat        | PL       | 3868   |\n",
    "| Suc khoe         | SK       | 3384   |\n",
    "| The gioi         | TG       | 2898   |\n",
    "| The thao         | TT       | 5298   |\n",
    "| Van hoa          | VH       | 3080   |\n",
    "| Vi tinh          | VT       | 2481   |\n",
    "\n",
    "Total 33759\n",
    "\n",
    "**_Test_**\n",
    "\n",
    "| Topic            | Topic ID | #files |\n",
    "| ---------------- | -------- | ------ |\n",
    "| Chinh tri Xa hoi | XH       | 7567   |\n",
    "| Doi song         | DS       | 2036   |\n",
    "| Khoa hoc         | KH       | 2096   |\n",
    "| Kinh doanh       | KD       | 5276   |\n",
    "| Phap luat        | PL       | 3788   |\n",
    "| Suc khoe         | SK       | 5417   |\n",
    "| The gioi         | TG       | 6716   |\n",
    "| The thao         | TT       | 6667   |\n",
    "| Van hoa          | VH       | 6250   |\n",
    "| Vi tinh          | VT       | 4560   |\n",
    "\n",
    "Total 50373\n",
    "\n",
    "Dữ liệu trong các file cào từ:\n",
    "\n",
    "- DS*VNE*(...) : VnExpress news agency (http://vnexpress.net/)\n",
    "- DS*TT*(...): Youth news agency (http://tuoitre.vn/)\n",
    "- DS*TN*(...): Thanh Nien news agency (http://thanhnien.vn/)\n",
    "- DS*NLD*(...): Nguoi Lao Dong news agency (http://nld.com.vn/)\n",
    "\n",
    "File zip chứa toàn bộ file, tên mỗi file là [Nhãn]\\_[Báo]\\_(STT).txt VD: XH_NLD_(3675).txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "\n",
    "def extract_data_from_zip(zip_file_path):\n",
    "    result = []\n",
    "    with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "        file_list = zip_ref.namelist()  # Lấy danh sách tên các file trong zip\n",
    "\n",
    "        for file_name in file_list:\n",
    "            if file_name.endswith('.txt'):  # Chỉ xử lý các file có đuôi .txt\n",
    "                with zip_ref.open(file_name) as file:\n",
    "                    data = file.read().decode('utf-16-le')  # Nó được mã hoá bằng utf-16-le\n",
    "                    # nội dung file có dạng \\ufeff<content>, đôi khi có dấu cách ở đầu và cuối\n",
    "                    content = data[1:].strip()\n",
    "                    words = content.split(' ')\n",
    "                    content = ' '.join(words)#(words[:min(len(words), 20)])\n",
    "                    label = file_name.split('/')[2].split('_')[0]\n",
    "                    result.append([content, label])\n",
    "    return result"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tiền xử lý dữ liệu"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Xoá HTML\n",
    "\n",
    "Dữ liệu được thu thập từ các website đôi khi vẫn còn sót lại các đoạn mã HTML. Các mã HTML code này là rác, chẳng những không có tác dụng cho việc phân loại mà còn làm kết quả phân loại văn bản bị kém đi. Do đó, cần phải loại bỏ các đoạn mã HTML này."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' This is an example nè'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "def remove_html(txt):\n",
    "    return re.sub(r'<[^>]*>', ' ', txt)\n",
    "\n",
    "txt = \"<p class=\\\"par\\\">This is an example</p>nè\"\n",
    "remove_html(txt)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chuẩn hoá Tiếng Việt\n",
    "\n",
    "- **Chuẩn hoá Unicode**: Hiện nay, có 2 loại mã Unicode được sử dụng phổ biến, Unicode tổ hợp và Unicode dựng sẵn. Hướng xử lý: Đưa về 1 chuẩn Unicode dựng sẵn (thằng này phổ biến hơn)\n",
    "- **Chuẩn hoán cách bỏ dấu**: Chuyển câu văn về cách gõ dấu kiểu cũ: dùng òa úy thay oà uý"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False False False True\n",
      "True True True True\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "uniChars = \"àáảãạâầấẩẫậăằắẳẵặèéẻẽẹêềếểễệđìíỉĩịòóỏõọôồốổỗộơờớởỡợùúủũụưừứửữựỳýỷỹỵÀÁẢÃẠÂẦẤẨẪẬĂẰẮẲẴẶÈÉẺẼẸÊỀẾỂỄỆĐÌÍỈĨỊÒÓỎÕỌÔỒỐỔỖỘƠỜỚỞỠỢÙÚỦŨỤƯỪỨỬỮỰỲÝỶỸỴÂĂĐÔƠƯ\"\n",
    "unsignChars = \"aaaaaaaaaaaaaaaaaeeeeeeeeeeediiiiiooooooooooooooooouuuuuuuuuuuyyyyyAAAAAAAAAAAAAAAAAEEEEEEEEEEEDIIIOOOOOOOOOOOOOOOOOOOUUUUUUUUUUUYYYYYAADOOU\"\n",
    "\n",
    "\n",
    "def loaddicchar():\n",
    "    dic = {}\n",
    "    char1252 = 'à|á|ả|ã|ạ|ầ|ấ|ẩ|ẫ|ậ|ằ|ắ|ẳ|ẵ|ặ|è|é|ẻ|ẽ|ẹ|ề|ế|ể|ễ|ệ|ì|í|ỉ|ĩ|ị|ò|ó|ỏ|õ|ọ|ồ|ố|ổ|ỗ|ộ|ờ|ớ|ở|ỡ|ợ|ù|ú|ủ|ũ|ụ|ừ|ứ|ử|ữ|ự|ỳ|ý|ỷ|ỹ|ỵ|À|Á|Ả|Ã|Ạ|Ầ|Ấ|Ẩ|Ẫ|Ậ|Ằ|Ắ|Ẳ|Ẵ|Ặ|È|É|Ẻ|Ẽ|Ẹ|Ề|Ế|Ể|Ễ|Ệ|Ì|Í|Ỉ|Ĩ|Ị|Ò|Ó|Ỏ|Õ|Ọ|Ồ|Ố|Ổ|Ỗ|Ộ|Ờ|Ớ|Ở|Ỡ|Ợ|Ù|Ú|Ủ|Ũ|Ụ|Ừ|Ứ|Ử|Ữ|Ự|Ỳ|Ý|Ỷ|Ỹ|Ỵ'.split(\n",
    "        '|')\n",
    "    charutf8 = \"à|á|ả|ã|ạ|ầ|ấ|ẩ|ẫ|ậ|ằ|ắ|ẳ|ẵ|ặ|è|é|ẻ|ẽ|ẹ|ề|ế|ể|ễ|ệ|ì|í|ỉ|ĩ|ị|ò|ó|ỏ|õ|ọ|ồ|ố|ổ|ỗ|ộ|ờ|ớ|ở|ỡ|ợ|ù|ú|ủ|ũ|ụ|ừ|ứ|ử|ữ|ự|ỳ|ý|ỷ|ỹ|ỵ|À|Á|Ả|Ã|Ạ|Ầ|Ấ|Ẩ|Ẫ|Ậ|Ằ|Ắ|Ẳ|Ẵ|Ặ|È|É|Ẻ|Ẽ|Ẹ|Ề|Ế|Ể|Ễ|Ệ|Ì|Í|Ỉ|Ĩ|Ị|Ò|Ó|Ỏ|Õ|Ọ|Ồ|Ố|Ổ|Ỗ|Ộ|Ờ|Ớ|Ở|Ỡ|Ợ|Ù|Ú|Ủ|Ũ|Ụ|Ừ|Ứ|Ử|Ữ|Ự|Ỳ|Ý|Ỷ|Ỹ|Ỵ\".split(\n",
    "        '|')\n",
    "    for i in range(len(char1252)):\n",
    "        dic[char1252[i]] = charutf8[i]\n",
    "    return dic\n",
    "\n",
    "dicchar = loaddicchar()\n",
    "\n",
    "def convert_unicode(txt):\n",
    "    return re.sub(\n",
    "        r'à|á|ả|ã|ạ|ầ|ấ|ẩ|ẫ|ậ|ằ|ắ|ẳ|ẵ|ặ|è|é|ẻ|ẽ|ẹ|ề|ế|ể|ễ|ệ|ì|í|ỉ|ĩ|ị|ò|ó|ỏ|õ|ọ|ồ|ố|ổ|ỗ|ộ|ờ|ớ|ở|ỡ|ợ|ù|ú|ủ|ũ|ụ|ừ|ứ|ử|ữ|ự|ỳ|ý|ỷ|ỹ|ỵ|À|Á|Ả|Ã|Ạ|Ầ|Ấ|Ẩ|Ẫ|Ậ|Ằ|Ắ|Ẳ|Ẵ|Ặ|È|É|Ẻ|Ẽ|Ẹ|Ề|Ế|Ể|Ễ|Ệ|Ì|Í|Ỉ|Ĩ|Ị|Ò|Ó|Ỏ|Õ|Ọ|Ồ|Ố|Ổ|Ỗ|Ộ|Ờ|Ớ|Ở|Ỡ|Ợ|Ù|Ú|Ủ|Ũ|Ụ|Ừ|Ứ|Ử|Ữ|Ự|Ỳ|Ý|Ỷ|Ỹ|Ỵ',\n",
    "        lambda x: dicchar[x.group()], txt)\n",
    "    \n",
    "dict_map = {\"òa\": \"oà\", \"Òa\": \"Oà\", \"ÒA\": \"OÀ\", \"óa\": \"oá\", \"Óa\": \"Oá\", \"ÓA\": \"OÁ\", \"ỏa\": \"oả\", \"Ỏa\": \"Oả\", \"ỎA\": \"OẢ\", \"õa\": \"oã\", \"Õa\": \"Oã\", \"ÕA\": \"OÃ\", \"ọa\": \"oạ\", \"Ọa\": \"Oạ\", \"ỌA\": \"OẠ\", \"òe\": \"oè\", \"Òe\": \"Oè\", \"ÒE\": \"OÈ\", \"óe\": \"oé\", \"Óe\": \"Oé\", \"ÓE\": \"OÉ\", \"ỏe\": \"oẻ\", \"Ỏe\": \"Oẻ\", \"ỎE\": \"OẺ\", \"õe\": \"oẽ\", \"Õe\": \"Oẽ\", \"ÕE\": \"OẼ\", \"ọe\": \"oẹ\", \"Ọe\": \"Oẹ\", \"ỌE\": \"OẸ\", \"ùy\": \"uỳ\", \"Ùy\": \"Uỳ\", \"ÙY\": \"UỲ\", \"úy\": \"uý\", \"Úy\": \"Uý\", \"ÚY\": \"UÝ\", \"ủy\": \"uỷ\", \"Ủy\": \"Uỷ\", \"ỦY\": \"UỶ\", \"ũy\": \"uỹ\", \"Ũy\": \"Uỹ\", \"ŨY\": \"UỸ\", \"ụy\": \"uỵ\", \"Ụy\": \"Uỵ\", \"ỤY\": \"UỴ\",}\n",
    "\n",
    "def replace_all(text, dict_map):\n",
    "    for i, j in dict_map.items():\n",
    "        text = text.replace(i, j)\n",
    "    return text\n",
    "\n",
    "def vietnameseTextNormalizer(document):    \n",
    "    # xóa html code\n",
    "    document = remove_html(document)\n",
    "    # chuẩn hóa unicode\n",
    "    document = convert_unicode(document)\n",
    "    import unicodedata    \n",
    "    return replace_all(unicodedata.normalize('NFC', convert_unicode(document)), dict_map)\n",
    "\n",
    "str_utf8 = 'Anh Hòa, đang làm gì chị Thúy vậy, ăn qụyt phải không?' # Unicode (dựng sẵn - dấu theo ký tự)\n",
    "str_utf8_2 = 'Anh Hoà, đang làm gì chị Thuý vậy, ăn quỵt phải không?' # Unicode (dựng sẵn - dấu theo ký tự)\n",
    "str_com = 'Anh Hòa, đang làm gì chị Thúy vậy, ăn qụyt phải không?'  # Unicode composite (tổ hợp - dấu riêng)\n",
    "str_1252 = 'Anh Hòa, đang làm gì chị Thúy vậy, ăn qụyt phải không?' # Windows-1252 = Latin-1\n",
    "print(str_utf8 == str_utf8_2, str_utf8 == str_com, str_utf8 == str_1252, str_com == str_1252)\n",
    "str_utf8 = vietnameseTextNormalizer(str_utf8)\n",
    "str_utf8 = vietnameseTextNormalizer(str_utf8_2)\n",
    "str_com = vietnameseTextNormalizer(str_com)\n",
    "str_1252 = vietnameseTextNormalizer(str_1252)\n",
    "print(str_utf8 == str_utf8_2, str_utf8 == str_com, str_utf8 == str_1252, str_com == str_1252)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tách từ\n",
    "\n",
    "File từ điển các từ và cụm từ được tổng hợp từ các nguồn sau:\n",
    "- Viet74K.txt: https://github.com/undertheseanlp/underthesea/tree/main/underthesea/corpus/data\n",
    "- vi-vocab: https://github.com/vncorenlp/VnCoreNLP/tree/master/models/wordsegmenter\n",
    "- Thư mục Words - Danh mục từ của wordnet: https://github.com/zeloru/vietnamese-wordnet/tree/master\n",
    "\n",
    "Sau đó được xử lý để tạo thành 1 file từ điển duy nhất tên là [dic3.txt](data/dic3.txt) nhờ code từ: [Create_Data.ipynb](Create_Data.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Số lượng từ ghép và cụm từ trong vocab: 112343\n",
      "Số bộ vocab phân theo độ dài: 20\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "import re\n",
    "\n",
    "def syllablize(sentence): # Tách âm tiết cho một câu\n",
    "    word = '\\w+'\n",
    "    non_word = '[^\\w\\s]'\n",
    "    digits = '\\d+([\\.,_]\\d+)+'\n",
    "    \n",
    "    patterns = []\n",
    "    patterns.extend([word, non_word, digits])\n",
    "    patterns = f\"({'|'.join(patterns)})\"\n",
    "    \n",
    "    tokens = re.findall(patterns, sentence, re.UNICODE)\n",
    "    return [token[0] for token in tokens]\n",
    "\n",
    "# Tải từ trong từ điển\n",
    "with open('data/dic3.txt', encoding='utf8') as f:\n",
    "    vocab = f.read().split('\\n')\n",
    "\n",
    "# Xây dựng từ điển vocabs theo độ dài từ\n",
    "vocabs = defaultdict(dict) # dùng list chậm hơn so với dict khoảng 25 lần / dict tìm keys bằng hash table\n",
    "for word in vocab:\n",
    "    vocabs[len(word.split())][word.lower()] = True\n",
    "\n",
    "print('Số lượng từ ghép và cụm từ trong vocab:', len(vocab))\n",
    "print('Số bộ vocab phân theo độ dài:', len(vocabs))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ghép các từ đơn thành từ ghép hoặc cụm từ nhờ từ điển\n",
    "\n",
    "- **Ghép từ đơn thành từ ghép**: VD: 'tập trung' -> 'tập_trung'\n",
    "- **Ghép từ đơn thành cụm từ**: VD: 'sự thực hiện' -> 'sự_thực_hiện'\n",
    "\n",
    "Việc này sẽ giúp cho việc tách từ được chính xác hơn, đồng thời giúp cho việc tạo từ điển để mã hoá văn bản dễ dàng hơn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'nhưng sự_thực_hiện vẫn_còn chưa phù_hợp'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def longest_matching(sentence, vocabs):\n",
    "  words = syllablize(sentence) # tách âm tiết cho câu\n",
    "  result = []\n",
    "  i = len(words)-1 # index của từ hiện tại\n",
    "  while i > -1: \n",
    "    word = '' \n",
    "    # tìm kiếm trong từ điển theo chiều dài của từ ưu tiên từ dài trước\n",
    "    for j in range(max(i-10,0), i+1): # tối đa 10 từ, dài quá thôi khỏi, chậm lắm, dài quá đa phần là thành ngữ, các trường hợp đặc biệt thôi\n",
    "      ls_word = words[j:i+1]\n",
    "      word = ' '.join(ls_word)\n",
    "      # xem thử có trong từ điển không\n",
    "      if word.lower() in vocabs.get(i-j+1, []):\n",
    "        i = j\n",
    "        break\n",
    "    result.append(word)\n",
    "    i-=1\n",
    "  return result[::-1] # đảo ngược lại vì quá trình tìm kiếm từ cuối câu lên đầu câu\n",
    "\n",
    "def tokenize_sentences(sentence):\n",
    "    return ' '.join([x.replace(' ','_') for x in longest_matching(sentence, vocabs)])\n",
    "\n",
    "tokenize_sentences('nhưng sự thực hiện vẫn còn chưa phù hợp')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loại bỏ stopword\n",
    "\n",
    "Stopword là những từ không có ý nghĩa trong văn bản, thường là những từ phổ biến trong ngôn ngữ. VD: 'là', 'có', 'được', 'và', 'của', 'các', 'cũng', 'để', 'trong', ...\n",
    "\n",
    "Từ điển stopword được lấy từ: https://github.com/huylv69/Vietnamese-Text-Classification\n",
    "\n",
    "Từ điển stopword có thể được tạo bằng cách thống kê các từ có trong tập train + test và xem từ nào có tần suất xuất hiện cao nhất thì nó là stopword. Tuy nhiên, cách này có thể sẽ bỏ sót một số từ không phổ biến nhưng lại không có ý nghĩa trong văn bản, nhất là các từ có cùng nghĩa với các stopword thông dụng. Có thể xem code thực tế trong file [Create_Data.ipynb](Create_Data.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'têu đề bài_báo có_vẻ_như hoà thuý có_thể_là ăn_quỵt'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('data/stopwords-nlp-vi.txt', encoding='utf8') as f:\n",
    "    stopword = f.read().replace(' ','_').split('\\n')\n",
    "\n",
    "stopword = set(stopword)\n",
    "\n",
    "def remove_stopwords(line):\n",
    "    words = []\n",
    "    for word in line.strip().split():\n",
    "        if word not in stopword:\n",
    "            words.append(word)\n",
    "    return ' '.join(words)\n",
    "\n",
    "remove_stopwords('têu đề bài_báo có_vẻ_như anh hoà đang làm_gì đó với chị thuý vậy có_thể_là ăn_quỵt có_phải không')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hoàn thiện tiền xử lý"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'têu đề bài_báo có_vẻ_như hoà thuý có_thể_là ăn_quỵt'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def text_preprocess(document):\n",
    "    # chuẩn hóa cách gõ dấu tiếng Việt\n",
    "    document = vietnameseTextNormalizer(document)\n",
    "    # tách từ\n",
    "    document = tokenize_sentences(document)\n",
    "    # đưa về lower\n",
    "    document = document.lower()\n",
    "    # xóa các ký tự không cần thiết\n",
    "    document = re.sub(r'[^\\s\\wáàảãạăắằẳẵặâấầẩẫậéèẻẽẹêếềểễệóòỏõọôốồổỗộơớờởỡợíìỉĩịúùủũụưứừửữựýỳỷỹỵđ_]',' ',document)\n",
    "    # xóa stopwords\n",
    "    document = remove_stopwords(document)\n",
    "    # xóa khoảng trắng thừa\n",
    "    document = re.sub(r'\\s+', ' ', document).strip()\n",
    "    return document\n",
    "\n",
    "text_preprocess('<p class=\\\"par\\\">Têu đề bài báo:</p>Có vẻ như Anh Hòa, đang làm gì đó với chị Thúy vậy, có thể là ăn qụyt có phải không?')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Xử lý toàn bộ tập train và test trước khi thực hiện đưa vào mô hình"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33759"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = extract_data_from_zip('data/Train_Full.zip')\n",
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50373"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = extract_data_from_zip('data/Test_Full.zip')\n",
    "len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 33759/33759 [04:59<00:00, 112.68it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "# Xử lý dữ liệu train cho chuẩn đã\n",
    "for i in tqdm(range(len(train_data))):\n",
    "    train_data[i][0] = text_preprocess(train_data[i][0])\n",
    "## loại bỏ các dòng trống\n",
    "train_data = [x for x in train_data if x[0] != '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50373/50373 [07:31<00:00, 111.63it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "# Xử lý dữ liệu Test cho chuẩn đã\n",
    "for i in tqdm(range(len(test_data))):\n",
    "    test_data[i][0] = text_preprocess(test_data[i][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lưu lại dữ liệu đã xử lý\n",
    "import pickle\n",
    "with open('train_data.pkl', 'wb') as f:\n",
    "    pickle.dump(train_data, f)\n",
    "with open('test_data.pkl', 'wb') as f:\n",
    "    pickle.dump(test_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load lại dữ liệu đã xử lý\n",
    "import pickle\n",
    "with open('train_data.pkl', 'rb') as f:\n",
    "    train_data = pickle.load(f)\n",
    "with open('test_data.pkl', 'rb') as f:\n",
    "    test_data = pickle.load(f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tạo từ điển labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'DS', 'KD', 'KH', 'PL', 'SK', 'TG', 'TT', 'VH', 'VT', 'XH'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = [x[1] for x in train_data]\n",
    "labels = set(labels)\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Văn hoá, Kinh doanh, Thế giới, Thể thao, Pháp luật, Sức khoẻ, Đời sống, Chính trị xã hội, Khoa học, Vi tính\n"
     ]
    }
   ],
   "source": [
    "label_dict={'XH':(0,'Chính trị xã hội'), \n",
    "            'DS':(1,'Đời sống'), \n",
    "            'KH':(2,'Khoa học'), \n",
    "            'KD':(3,'Kinh doanh'), \n",
    "            'PL':(4,'Pháp luật'), \n",
    "            'SK':(5,'Sức khoẻ'), \n",
    "            'TG':(6,'Thế giới'), \n",
    "            'TT':(7,'Thể thao'), \n",
    "            'VH':(8,'Văn hoá'), \n",
    "            'VT':(9,'Vi tính') }\n",
    "print(*[label_dict[x][1] for x in labels], sep = ', ')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Xây dựng mô hình phân loại văn bản"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Kiểm tra môi trường lập trình và sử dụng GPU nếu có"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch is using GPU.\n",
      "Current GPU device: 0\n",
      "Number of available GPUs: 1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"PyTorch is using GPU.\")\n",
    "    device = torch.device(\"cuda\")  # Chọn GPU làm thiết bị tính toán\n",
    "    print(f\"Current GPU device: {torch.cuda.current_device()}\")\n",
    "    print(f\"Number of available GPUs: {torch.cuda.device_count()}\")\n",
    "else:\n",
    "    print(\"PyTorch is using CPU.\")\n",
    "    device = torch.device(\"cpu\")  # Chọn CPU là thiết bị tính toán\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Xây dựng bộ từ điển và mã hoá văn bản thành số\n",
    "\n",
    "Để trực tiếp văn bản thì không thể đưa vào mô hình được, do đó cần phải mã hoá văn bản thành số. Để làm được việc này, cần phải xây dựng bộ từ điển. Bộ từ điển này sẽ được xây dựng dựa trên tập train. Các từ có trong tập train sẽ được đưa vào bộ từ điển. Sau đó, các input sẽ được mã hoá thành số dựa trên bộ từ điển này."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'trần': 81, 'ngành': 82, 'nam': 83, 'trung_quốc': 84, 'phát_hiện': 85, 'bệnh': 86, '2006': 87, 'sản_phẩm': 88, '18': 89, 'dịch_vụ': 90, 'bóng_đá': 91, 'tiền_đạo': 92, 'chồng': 93, 'tuần': 94, 'yêu': 95, 'hệ_thống': 96, 'quốc_tế': 97, 'khi_đó': 98, 'người_ta': 99, 'người_dân': 100}\n"
     ]
    }
   ],
   "source": [
    "def create_vocab_dict(data):\n",
    "    word_counts = defaultdict(int)\n",
    "    for text, label in data:\n",
    "        for word in text.split():\n",
    "            word_counts[word] += 1\n",
    "\n",
    "    sorted_words = sorted(word_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "    vocab_dict = {}\n",
    "    for i, (word, count) in enumerate(sorted_words):\n",
    "        vocab_dict[word] = i + 1\n",
    "    \n",
    "    vocab_dict['<unk>'] = 0\n",
    "    return vocab_dict\n",
    "\n",
    "vocab_dict = create_vocab_dict(train_data)\n",
    "print(dict(list(vocab_dict.items())[80:100]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1003,  1043,   134,   703,   441,    11,    22,  1043,  1904,   922,\n",
       "         9584,   261,  1085,   145,  2207,   132,  1435,   751,  7367,    61,\n",
       "        23074,  1053, 32473,     9,   360,  1043, 32474,  3182,   134,   703,\n",
       "          441, 80104, 80105, 31063, 31063,  2541,   981,  3372, 40108,    24,\n",
       "         7020,  3301,    71,   825,   145,   703, 80106,  2650,   145,  1920,\n",
       "          512,   294,  1370,  4179,  1043,  6536,  3141,  1043,  3759,   490,\n",
       "          134,   703, 14874,     2, 12299,  3337,    11,    22])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sentence_to_num(sentence, vocab):\n",
    "    tokens = sentence.split()\n",
    "    sentence_indexes = [vocab.get(token, vocab['<unk>']) for token in tokens]\n",
    "    return torch.tensor(sentence_indexes)\n",
    "\n",
    "sentence_to_num(train_data[2][0], vocab_dict)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Thiết lập các thông số cho mô hình\n",
    "Trong mô hình phân loại văn bản thì `vocab_size`, `embedding_dim`, `hidden_size`, và `num_classes` là các thông số quan trọng cần được xác định trước khi xây dựng mô hình:\n",
    "\n",
    "1. `vocab_size` là kích thước của từ điển (vocabulary) trong mô hình phân loại văn bản. Nó đại diện cho số lượng từ duy nhất có trong tập dữ liệu huấn luyện. Khi xây dựng mô hình, mỗi từ sẽ được biểu diễn bằng một chỉ số số nguyên từ 0 đến `vocab_size - 1`. `vocab_size` có thể được tính toán tự động bằng cách đếm từ.\n",
    "\n",
    "2. `embedding_dim`: Đây là số chiều của không gian nhúng (embedding space) trong mô hình. Trong quá trình huấn luyện, các từ trong từ điển sẽ được biểu diễn bằng các vectơ có kích thước `embedding_dim`. Số chiều này cần được chọn sao cho đủ lớn để mô hình có thể học được các đặc trưng quan trọng của văn bản, nhưng cũng không quá lớn để tránh tăng quá nhiều tham số và tốn thời gian huấn luyện.Thông thường, kích thước không gian nhúng từ 100 đến 300 chiều đã được sử dụng hiệu quả trong nhiều nhiệm vụ phân loại văn bản.\n",
    "\n",
    "3. `hidden_size`: Đây là số lượng đơn vị ẩn trong mạng LSTM (Long Short-Term Memory) hoặc các mạng RNN (Recurrent Neural Network) khác. `hidden_size` ảnh hưởng đến khả năng mô hình học các mẫu dữ liệu phức tạp. Nếu `hidden_size` lớn, mô hình có khả năng học các mẫu phức tạp hơn, nhưng đồng thời tăng cường độ phức tạp của mô hình và thời gian huấn luyện. Giá trị thông thường cho `hidden_size` trong mạng LSTM hoặc RNN là 100, 200 hoặc 300. Tuy nhiên, nếu tập dữ liệu lớn hoặc bài toán phân loại phức tạp hơn, có thể cần tăng giá trị này để mô hình có khả năng học mẫu phức tạp hơn.\n",
    "\n",
    "4. `num_classes`: Đây là số lượng lớp trong bài toán phân loại. Đối với bài toán phân loại văn bản, `num_classes` sẽ là số lượng nhãn khác nhau mà chúng ta muốn mô hình phân loại các văn bản vào. Ví dụ, nếu ta có 3 nhãn: \"positive\", \"negative\", và \"neutral\", thì `num_classes` sẽ là 3.\n",
    "\n",
    "Việc lựa chọn các giá trị tối ưu cho `embedding_dim`, `hidden_size`, và `num_classes` phụ thuộc vào bài toán cụ thể và dữ liệu đang làm việc. Thông thường, các giá trị này được chọn dựa trên kinh nghiệm thực tế và thử nghiệm. Trong quá trình huấn luyện, bạn có thể điều chỉnh các giá trị này và theo dõi hiệu suất của mô hình để tìm ra các giá trị phù hợp nhất."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "122620 10\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(vocab_dict)\n",
    "embedding_dim = 100 # RAM GPU 16GB thì mới lấy được 1000 (Hội tụ nhanh nhưng độ chính xác không cải thiện mấy 85%), nếu không tràn RAM, máy yếu, RAM GPU 4GB thì lấy 100 (độ chính xác 84%)\n",
    "hidden_size = 128\n",
    "num_classes = len(labels)\n",
    "\n",
    "print(vocab_size, num_classes)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tạo lớp Dataset để đóng gói dữ liệu\n",
    "Trong `PyTorch`, `Dataset` là một lớp trừu tượng để biểu diễn bộ dữ liệu. Lớp `Dataset` cung cấp một giao diện chuẩn cho việc truy cập đến các mẫu dữ liệu trong bộ dữ liệu.\n",
    "\n",
    "Một lớp `Dataset` thường được sử dụng để đóng gói dữ liệu và các phép xử lý trước khi đưa vào mô hình. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "# Lớp tập dữ liệu tùy chỉnh\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        text, label = self.data[index]\n",
    "        x = sentence_to_num(text, vocab_dict) # Chuyển câu văn thành dãy số\n",
    "        index = label_dict[label][0] # Chuyển nhãn về dạng số\n",
    "        y = F.one_hot(torch.tensor(index), num_classes=num_classes).float() # Chuyển nhãn về dạng one-hot vector\n",
    "        return x, y # Trả về cặp giá trị đầu vào và nhãn tương ứng cho quá trình huấn luyện\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tạo lớp mô tả mạng neural\n",
    "Mô hình `TextClassifier` dựa trên mạng LSTM để phân loại văn bản. Mô hình này bao gồm các thành phần chính sau:\n",
    "\n",
    "- Lớp nhúng (embedding layer): Lớp nhúng này chuyển đổi các từ trong câu thành các vectơ số thực có số chiều bằng `embedding_dim`. Lớp nhúng được định nghĩa bằng lớp `nn.Embedding()` của PyTorch.\n",
    "\n",
    "- Mạng LSTM: Mạng LSTM được sử dụng để học các đặc trưng phức tạp từ các vectơ nhúng của các từ trong câu. Để định nghĩa mạng LSTM, chúng ta sử dụng lớp `nn.LSTM()` của PyTorch.\n",
    "\n",
    "- Lớp kết nối (fully connected layer): Lớp này được sử dụng để ánh xạ đầu ra của mạng LSTM sang đầu ra cuối cùng với số chiều bằng `num_classes`. Lớp này được định nghĩa bằng lớp `nn.Linear()` của PyTorch.\n",
    "\n",
    "Phương thức `forward` của mô hình được sử dụng để tính toán đầu ra của mô hình khi cho vào một batch các đầu vào `inputs` kèm theo độ dài `lengths` của các đầu vào.\n",
    "\n",
    "**Lưu ý quan trọng:**  Mặc định, hàm `nn.CrossEntropyLoss()` sẽ tính toán `Softmax`. Vì vậy, không cần phải sử dụng lớp `nn.Softmax()` trong mô hình. Nếu sử dụng lớp `nn.Softmax()` trong mô hình, kết quả sẽ không chính xác, mô hình thậm chí sẽ không hội tụ được."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class TextClassifier(nn.Module):\n",
    "    def __init__(self, num_classes, vocab_size, embedding_dim, hidden_size):\n",
    "        super(TextClassifier, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "        \n",
    "    def forward(self, inputs, lengths):\n",
    "        embedded = self.embedding(inputs)\n",
    "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, lengths, batch_first=True, enforce_sorted=False)\n",
    "        packed_output, (h_n, c_n) = self.lstm(packed_embedded)\n",
    "        output, output_lengths = nn.utils.rnn.pad_packed_sequence(packed_output, batch_first=True)\n",
    "        last_output = torch.zeros(output.size(0), output.size(2)).to(inputs.device)\n",
    "        for i in range(len(output)):\n",
    "            last_output[i] = output[i, lengths[i]-1, :]\n",
    "        logits = self.fc(last_output)\n",
    "        return logits"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chuẩn bị dữ liệu và các tham số huấn luyện.\n",
    "\n",
    "Trước khi huấn luyện mô hình, chúng ta cần cấu hình các siêu tham số cho quá trình huấn luyện, bao gồm: `batch_size`, `epochs` và `learning_rate`.\n",
    "\n",
    "- `batch_size`: Đây là số lượng mẫu dữ liệu được đưa vào mô hình trong mỗi lần huấn luyện. Khi huấn luyện mô hình, dữ liệu sẽ được chia thành các batch có kích thước bằng `batch_size`. Các batch này sẽ được đưa vào mô hình tuần tự để tính toán đầu ra và tính toán độ lỗi. Sau đó, các tham số của mô hình sẽ được cập nhật dựa trên độ lỗi của các batch này. Giá trị `batch_size` thường được chọn là một số lớn hơn 1 để tăng tốc độ huấn luyện và giảm bộ nhớ sử dụng.\n",
    "\n",
    "- `epochs`: Đây là số lượng lần mà toàn bộ dữ liệu được đưa vào mô hình để huấn luyện. \n",
    "\n",
    "- `learning_rate`: Đây là giá trị học tập của mô hình. Giá trị này ảnh hưởng đến tốc độ hội tụ của mô hình. Nếu `learning_rate` quá lớn, mô hình có thể không hội tụ được. Ngược lại, nếu `learning_rate` quá nhỏ, mô hình sẽ hội tụ rất chậm. Giá trị thông thường cho `learning_rate` là 0.001.\n",
    "\n",
    "Sau đó, chúng ta chuẩn bị dữ liệu cho quá trình huấn luyện bằng cách tạo `CustomDataset` từ `train_data` và `test_data`, các dữ liệu được trộn ngẫu nhiên nhờ tham số `shuffle=true`. \n",
    "\n",
    "Trong đó hàm `collate_fn` để ghép nối các mẫu dữ liệu trong mỗi batch thành 1 tensor kèm với thông tin về chiều dài mỗi mẫu dữ liệu. Hàm `collate_fn` này sẽ chuyển đổi các mẫu dữ liệu trong mỗi batch thành một tensor đầu vào `input_tensor`, một tensor đầu ra `label_tensor` và một danh sách các độ dài tương ứng với mỗi mẫu dữ liệu trong batch.\n",
    "\n",
    "Sau đó, chúng ta khởi tạo mô hình `TextClassifier` và tối ưu hóa mô hình bằng cách sử dụng hàm mất mát `CrossEntropyLoss` và thuật toán tối ưu hóa `Adam`. Mô hình được đưa lên thiết bị `device` trước khi huấn luyện."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cấu hình\n",
    "batch_size = 100\n",
    "epochs = 10\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Chuẩn bị dữ liệu\n",
    "train_dataset = CustomDataset(train_data)\n",
    "test_dataset = CustomDataset(test_data)\n",
    "\n",
    "from torch.nn.utils.rnn import pack_sequence\n",
    "\n",
    "def collate_fn(batch):\n",
    "    # split the batch into inputs and labels\n",
    "    inputs, labels = zip(*batch)\n",
    "\n",
    "    # get the lengths of the inputs\n",
    "    lengths = [len(sample) for sample in inputs]\n",
    "\n",
    "    # convert inputs to a PyTorch tensor\n",
    "    input_tensor = torch.zeros(len(inputs), max(lengths)).long()\n",
    "    for i, sample in enumerate(inputs):\n",
    "        input_tensor[i, :lengths[i]] = sample.clone().detach() # tạo một bản sao của tensor gốc mà không bị tính toán gradient \n",
    "    # convert the labels to a PyTorch tensor\n",
    "    label_tensor = torch.stack(labels)\n",
    "    # return a tuple of the inputs, the lengths, and the labels\n",
    "    return input_tensor, lengths, label_tensor\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "# Khởi tạo mô hình và tối ưu hóa\n",
    "model = TextClassifier(num_classes, vocab_size, embedding_dim, hidden_size).to(device)\n",
    "criterion = nn.CrossEntropyLoss() # Mặc định, hàm nn.CrossEntropyLoss() sẽ tính toán softmax. Vì vậy, không cần phải sử dụng lớp nn.Softmax() trong mô hình.\n",
    "optimizer = optim.AdamW(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Huấn luyện mô hình\n",
    "Sử dụng `model.train()` để đánh dấu rằng mô hình đang được huấn luyện.\n",
    "\n",
    "Sử dụng một vòng lặp lớn để duyệt qua các `epoch`. Trong mỗi `epoch`, chúng ta sử dụng `train_loader` để lấy ra các `batch` dữ liệu để huấn luyện mô hình.\n",
    "\n",
    "Trong từng `batch` thực hiện các công việc sau:\n",
    "- Đưa dữ liệu vào thiết bị `device`, `lengths` không được lên được do hàm `pack_padded_sequence` ở  `forward` yêu cầu `lengths` nằm ở CPU\n",
    "- Sử dụng `optimizer.zero_grad()` để xóa các gradient tích lũy của các tham số mô hình. \n",
    "- Tính toán đầu ra của mô hình `outputs` bằng cách đưa vào đầu vào `inputs` và độ dài các chuỗi đầu vào `lengths`. \n",
    "- Tính giá trị loss function giữa đầu ra của mô hình và nhãn thực tế `labels` bằng hàm `criterion`.\n",
    "- Tính gradient của loss function bằng phương thức `backward()`, \n",
    "- Sử dụng `optimizer.step()` để cập nhật các tham số của mô hình dựa trên gradient tính được. \n",
    "- Lưu giá trị của loss function vào danh sách `loss_values` để theo dõi quá trình huấn luyện của mô hình."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch:338/338 - Epoch:10/10: 100%|██████████| 10/10 [29:54<00:00, 179.42s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "# Huấn luyện\n",
    "num_batches = len(train_loader)\n",
    "loss_values = []  # Khởi tạo danh sách để lưu trữ giá trị loss\n",
    "model.train()\n",
    "for epoch in (pbar := tqdm(range(epochs))):\n",
    "    counter = 1\n",
    "    for inputs, lengths, labels in train_loader:\n",
    "        pbar.set_description(f\"Batch:{counter}/{num_batches} - Epoch:{epoch+1}/{epochs}\")\n",
    "        counter += 1\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs, lengths)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_values.append(loss.item())  # Lưu trữ giá trị loss vào danh sách"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_values[::-1]\n",
    "# lưu loss vào file sau này load lại\n",
    "import pickle\n",
    "with open('loss_values.pkl', 'wb') as f:\n",
    "    pickle.dump(loss_values, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lưu mô hình\n",
    "torch.save(model.state_dict(), 'model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# # Tải loss từ file\n",
    "# import pickle\n",
    "# with open('loss_values.pkl', 'rb') as f:  \n",
    "#     loss_values = pickle.load(f)\n",
    "# plt.plot(loss_values)  # Vẽ đồ thị loss\n",
    "# plt.xlabel('Iteration')\n",
    "# plt.ylabel('Loss')\n",
    "# plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Đánh giá mô hình\n",
    "\n",
    "- Sử dụng `model.eval()` để đánh dấu rằng mô hình đang được đánh giá, chứ không phải đang được huấn luyện. \n",
    "- Sử dụng `torch.no_grad()` để tắt tính toán gradient trong quá trình đánh giá, giúp tiết kiệm bộ nhớ và tăng tốc độ tính toán.\n",
    "- Sử dụng `test_loader` để lấy ra các batch dữ liệu để đánh giá mô hình. \n",
    "- Với từng `batch` dữ liệu, chúng ta thực hiện các công việc sau:\n",
    "    - Đưa dữ liệu vào thiết bị `device`, \n",
    "    - Tính toán đầu ra của mô hình `outputs` bằng cách đưa vào đầu vào `inputs` và độ dài các chuỗi đầu vào `lengths`. \n",
    "    - Sử dụng hàm `torch.max()` để lấy ra nhãn dự đoán của mô hình `predicted` và so sánh với nhãn thực tế `lslabels` để tính toán số lượng dự đoán chính xác `correct` và tổng số lượng mẫu dữ liệu `total`.\n",
    "- Cuối cùng, tính độ chính xác trung bình của mô hình trên tập dữ liệu kiểm tra `accuracy` bằng cách tính tỷ lệ phần trăm giữa số lượng dự đoán chính xác và tổng số lượng mẫu dữ liệu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 84.04105373910626%\n"
     ]
    }
   ],
   "source": [
    "# Tải mô hình\n",
    "model = TextClassifier(num_classes, vocab_size, embedding_dim, hidden_size).to(device)\n",
    "model.load_state_dict(torch.load('model.pth'))\n",
    "\n",
    "# Đánh giá mô hình\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for inputs, lengths, labels in test_loader:\n",
    "        \n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(inputs, lengths)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        _, lslabels = torch.max(labels, 1)\n",
    "        total += lslabels.size(0)\n",
    "        correct += (predicted == lslabels).sum().item()\n",
    "    \n",
    "    accuracy = 100 * correct / total\n",
    "    print(f\"Accuracy: {accuracy}%\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sử dụng mô hình để phân loại văn bản\n",
    "\n",
    "Đây là một chương trình Python được sử dụng để dự đoán thể loại của một đoạn văn bản bất kỳ. Chương trình sử dụng một mô hình học sâu (neural network) để dự đoán thể loại của văn bản đầu vào. Mô hình này đã được đào tạo trước và được sử dụng để phân loại văn bản vào một trong các nhãn được định nghĩa trước (label_dict).\n",
    "\n",
    "Cụ thể:\n",
    "\n",
    "- Hàm `normalize_percent(data)` nhận đầu vào là một mảng (array) `data` và trả về một mảng mới `data_normalized` được chuẩn hóa về phần trăm (%). Hàm sử dụng công thức chuẩn hóa min-max để đưa các giá trị trong `data` về đoạn [0, 1], rồi nhân với 100 để đưa về phần trăm.\n",
    "\n",
    "- Hàm `predict(text)` nhận đầu vào là một đoạn văn bản `text` và trả về một mảng `outputs` chứa xác suất dự đoán cho mỗi nhãn (label) trong `label_dict`. Hàm này sử dụng mô hình đã được đào tạo trước đó (được lưu trong biến `model`) để tính toán xác suất dự đoán. Đầu vào của mô hình là một tensor được chuyển đổi từ đoạn văn bản `text` bằng hàm `sentence_to_num()` và được mã hóa dưới dạng số (sử dụng `vocab_dict` để mã hóa). Kết quả đầu ra của mô hình là một tensor chứa xác suất cho mỗi nhãn.\n",
    "\n",
    "- Hàm `getLabel(outputs)` nhận đầu vào là một mảng `outputs` chứa xác suất dự đoán cho mỗi nhãn và trả về tên nhãn được dự đoán là chính xác nhất. Hàm này sử dụng tensor `outputs` để lấy nhãn có xác suất cao nhất và trả về tên tương ứng của nhãn đó. Tên nhãn được lấy từ `label_dict` dựa trên mã số của nhãn.\n",
    "\n",
    "- Hàm `getLabels(outputs)` nhận đầu vào là một mảng `outputs` chứa xác suất dự đoán cho mỗi nhãn và trả về một danh sách các chuỗi chứa tên và xác suất của các nhãn. Hàm này sử dụng tensor `outputs` để lấy xác suất cho mỗi nhãn và chuẩn hóa xác suất đó về phần trăm bằng hàm `normalize_percent()`. Sau đó, hàm sử dụng `label_dict` để lấy tên của mỗi nhãn và kết hợp với xác suất tương ứng để tạo ra một chuỗi kết quả. Danh sách các chuỗi này được trả về.\n",
    "\n",
    "- Đoạn code cuối cùng sử dụng hàm `predict()` để dự đoán nhãn của một đoạn văn bản `test`. Kết quả dự đoán được trả về dưới dạng một mảng `outputs`. Sau đó, hàm `getLabels()` và `getLabel()` được sử dụng để lấy danh sách các chuỗi kết quả và tên nhãn được dự đoán chính xác nhất. Cuối cùng, đoạn văn bản `test` ban đầu và kết quả dự đoán được in ra màn hình."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_percent(data):\n",
    "    data_plus = data + data.min()\n",
    "    data_normalized = (data - data.min()) / (data.max() - data.min()) * 100\n",
    "    return data_normalized\n",
    "# Hàm trả về tên nhãn dự đoán\n",
    "def predict(text):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        inputs = sentence_to_num(text, vocab_dict).unsqueeze(0).to(device)\n",
    "        lengths = [inputs.size(1)]\n",
    "        outputs = model(inputs, lengths)\n",
    "        return normalize_percent(outputs)\n",
    "\n",
    "def getLabel(outputs):\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    result = [label_dict[x][1] for x in label_dict if label_dict[x][0] == predicted.item()]\n",
    "    return result[0]\n",
    "\n",
    "def getLabels(outputs):\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    data = outputs.data.tolist()[0]\n",
    "    dict_labels = [v[1] for k,v in label_dict.items()]\n",
    "    result = []\n",
    "    for i in range(len(label_dict)) :\n",
    "      result.append(f'-{dict_labels[i]}: {round(data[i])}%')\n",
    "    return result\n",
    "\n",
    "test = test_data[91][0]\n",
    "outputs = predict(test)\n",
    "print('Kết quả tính toán:')\n",
    "print(*getLabels(outputs), sep='\\n')\n",
    "print('Như vậy dự đoán là thuộc thể loại:', getLabel(outputs))\n",
    "print(test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "OpenCV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
