{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bài toán\n",
    "\n",
    "https://blog.luyencode.net/phan-loai-van-ban-tieng-viet/#bai-toan-phan-loai-van-ban\n",
    "\n",
    "- Input: một đoạn văn bản\n",
    "- Output: một trong các nhãn sau: 'thể thao', 'giáo dục', 'giải trí', 'kinh doanh', 'pháp luật', 'sức khỏe', 'số hóa', 'thế giới', 'thời sự', 'xe'\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thu thập dữ liệu"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Dữ liệu được lấy dừ https://github.com/duyvuleo/VNTC/tree/master/Data/10Topics/Ver1.1 với thông tin như sau:\n",
    "\n",
    "***Train***\n",
    "|Topic |\tTopic ID |\t#files |\n",
    "|--|--|--|\n",
    "| Chinh tri Xa hoi\t| XH |\t5219 |\n",
    "| Doi song\t| DS |\t\t3159 |\n",
    "| Khoa hoc\t| KH |\t\t1820 |\n",
    "| Kinh doanh\t| KD |\t\t2552 |\n",
    "| Phap luat\t| PL |\t\t3868 |\n",
    "| Suc khoe\t| SK |\t\t3384 |\n",
    "| The gioi\t| TG |\t\t2898 |\n",
    "| The thao\t| TT |\t\t5298 |\n",
    "| Van hoa \t| VH |\t\t3080 |\n",
    "| Vi tinh\t\t| VT |\t\t2481 |\n",
    "\n",
    "Total\t\t\t\t33759\n",
    "\n",
    "***Test***\n",
    "|Topic |\tTopic ID |\t#files |\n",
    "|--|--|--|\n",
    "| Chinh tri Xa hoi\t| XH |\t7567 |\n",
    "| Doi song\t| DS |\t\t2036 |\n",
    "| Khoa hoc\t| KH |\t\t2096 |\n",
    "| Kinh doanh\t| KD |\t\t5276 |\n",
    "| Phap luat\t| PL |\t\t3788 |\n",
    "| Suc khoe\t| SK |\t\t5417 |\n",
    "| The gioi\t| TG |\t\t6716 |\n",
    "| The thao\t| TT |\t\t6667 |\n",
    "| Van hoa\t\t| VH |\t\t6250 |\n",
    "| Vi tinh\t\t| VT |\t\t4560 |\n",
    "\n",
    "Total\t\t\t\t50373\n",
    "\n",
    "Tên file được đặt cào từ:\n",
    "\n",
    "+ DS_VNE_(...) : VnExpress news agency (http://vnexpress.net/)\n",
    "+ DS_TT_(...):  Youth news agency (http://tuoitre.vn/)\n",
    "+ DS_TN_(...): Thanh Nien news agency (http://thanhnien.vn/)\n",
    "+ DS_NLD_(...): Nguoi Lao Dong news agency (http://nld.com.vn/)\n",
    "\n",
    "File zip chứa toàn bộ file, tên mỗi file là Nhãn_Báo_ (STT).txt VD: XH_NLD_ (3675).txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "\n",
    "def extract_data_from_zip(zip_file_path):\n",
    "    result = []\n",
    "    with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "        file_list = zip_ref.namelist()  # Lấy danh sách tên các file trong zip\n",
    "\n",
    "        for file_name in file_list:\n",
    "            if file_name.endswith('.txt'):  # Chỉ xử lý các file có đuôi .txt\n",
    "                with zip_ref.open(file_name) as file:\n",
    "                    data = file.read().decode('utf-16-le')  # Nó được mã hoá bằng utf-16-le\n",
    "                    # nội dung file có dạng \\ufeff<content>, đôi khi có dấu cách ở đầu và cuối\n",
    "                    content = data[1:].strip()\n",
    "                    label = file_name.split('/')[2].split('_')[:2]\n",
    "                    result.append((content, label))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33759"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = extract_data_from_zip('data/Train_Full.zip')\n",
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50373"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = extract_data_from_zip('data/Test_Full.zip')\n",
    "len(test_data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tiền xử lý dữ liệu"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Xoá HTML\n",
    "\n",
    "Dữ liệu được thu thập từ các website đôi khi vẫn còn sót lại các đoạn mã HTML. Các mã HTML code này là rác, chẳng những không có tác dụng cho việc phân loại mà còn làm kết quả phân loại văn bản bị kém đi. Do đó, cần phải loại bỏ các đoạn mã HTML này."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This is an example'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "def remove_html(txt):\n",
    "    return re.sub(r'<[^>]*>', '', txt)\n",
    "\n",
    "txt = \"<p class=\\\"par\\\">This is an example</p>\"\n",
    "remove_html(txt)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chuẩn hoá Tiếng Việt\n",
    "\n",
    "- **Chuẩn hoá Unicode**: Hiện nay, có 2 loại mã Unicode được sử dụng phổ biến, Unicode tổ hợp và Unicode dựng sẵn. Hướng xử lý: Đưa về 1 chuẩn Unicode dựng sẵn (thằng này phổ biến hơn)\n",
    "- **Chuẩn hoán cách bỏ dấu**: Chuyển câu văn về cách gõ dấu kiểu cũ: dùng òa úy thay oà uý"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import re\n",
    "\n",
    "uniChars = \"àáảãạâầấẩẫậăằắẳẵặèéẻẽẹêềếểễệđìíỉĩịòóỏõọôồốổỗộơờớởỡợùúủũụưừứửữựỳýỷỹỵÀÁẢÃẠÂẦẤẨẪẬĂẰẮẲẴẶÈÉẺẼẸÊỀẾỂỄỆĐÌÍỈĨỊÒÓỎÕỌÔỒỐỔỖỘƠỜỚỞỠỢÙÚỦŨỤƯỪỨỬỮỰỲÝỶỸỴÂĂĐÔƠƯ\"\n",
    "unsignChars = \"aaaaaaaaaaaaaaaaaeeeeeeeeeeediiiiiooooooooooooooooouuuuuuuuuuuyyyyyAAAAAAAAAAAAAAAAAEEEEEEEEEEEDIIIOOOOOOOOOOOOOOOOOOOUUUUUUUUUUUYYYYYAADOOU\"\n",
    "\n",
    "\n",
    "def loaddicchar():\n",
    "    dic = {}\n",
    "    char1252 = 'à|á|ả|ã|ạ|ầ|ấ|ẩ|ẫ|ậ|ằ|ắ|ẳ|ẵ|ặ|è|é|ẻ|ẽ|ẹ|ề|ế|ể|ễ|ệ|ì|í|ỉ|ĩ|ị|ò|ó|ỏ|õ|ọ|ồ|ố|ổ|ỗ|ộ|ờ|ớ|ở|ỡ|ợ|ù|ú|ủ|ũ|ụ|ừ|ứ|ử|ữ|ự|ỳ|ý|ỷ|ỹ|ỵ|À|Á|Ả|Ã|Ạ|Ầ|Ấ|Ẩ|Ẫ|Ậ|Ằ|Ắ|Ẳ|Ẵ|Ặ|È|É|Ẻ|Ẽ|Ẹ|Ề|Ế|Ể|Ễ|Ệ|Ì|Í|Ỉ|Ĩ|Ị|Ò|Ó|Ỏ|Õ|Ọ|Ồ|Ố|Ổ|Ỗ|Ộ|Ờ|Ớ|Ở|Ỡ|Ợ|Ù|Ú|Ủ|Ũ|Ụ|Ừ|Ứ|Ử|Ữ|Ự|Ỳ|Ý|Ỷ|Ỹ|Ỵ'.split(\n",
    "        '|')\n",
    "    charutf8 = \"à|á|ả|ã|ạ|ầ|ấ|ẩ|ẫ|ậ|ằ|ắ|ẳ|ẵ|ặ|è|é|ẻ|ẽ|ẹ|ề|ế|ể|ễ|ệ|ì|í|ỉ|ĩ|ị|ò|ó|ỏ|õ|ọ|ồ|ố|ổ|ỗ|ộ|ờ|ớ|ở|ỡ|ợ|ù|ú|ủ|ũ|ụ|ừ|ứ|ử|ữ|ự|ỳ|ý|ỷ|ỹ|ỵ|À|Á|Ả|Ã|Ạ|Ầ|Ấ|Ẩ|Ẫ|Ậ|Ằ|Ắ|Ẳ|Ẵ|Ặ|È|É|Ẻ|Ẽ|Ẹ|Ề|Ế|Ể|Ễ|Ệ|Ì|Í|Ỉ|Ĩ|Ị|Ò|Ó|Ỏ|Õ|Ọ|Ồ|Ố|Ổ|Ỗ|Ộ|Ờ|Ớ|Ở|Ỡ|Ợ|Ù|Ú|Ủ|Ũ|Ụ|Ừ|Ứ|Ử|Ữ|Ự|Ỳ|Ý|Ỷ|Ỹ|Ỵ\".split(\n",
    "        '|')\n",
    "    for i in range(len(char1252)):\n",
    "        dic[char1252[i]] = charutf8[i]\n",
    "    return dic\n",
    "\n",
    "\n",
    "dicchar = loaddicchar()\n",
    "\n",
    "\n",
    "def convert_unicode(txt):\n",
    "    return re.sub(\n",
    "        r'à|á|ả|ã|ạ|ầ|ấ|ẩ|ẫ|ậ|ằ|ắ|ẳ|ẵ|ặ|è|é|ẻ|ẽ|ẹ|ề|ế|ể|ễ|ệ|ì|í|ỉ|ĩ|ị|ò|ó|ỏ|õ|ọ|ồ|ố|ổ|ỗ|ộ|ờ|ớ|ở|ỡ|ợ|ù|ú|ủ|ũ|ụ|ừ|ứ|ử|ữ|ự|ỳ|ý|ỷ|ỹ|ỵ|À|Á|Ả|Ã|Ạ|Ầ|Ấ|Ẩ|Ẫ|Ậ|Ằ|Ắ|Ẳ|Ẵ|Ặ|È|É|Ẻ|Ẽ|Ẹ|Ề|Ế|Ể|Ễ|Ệ|Ì|Í|Ỉ|Ĩ|Ị|Ò|Ó|Ỏ|Õ|Ọ|Ồ|Ố|Ổ|Ỗ|Ộ|Ờ|Ớ|Ở|Ỡ|Ợ|Ù|Ú|Ủ|Ũ|Ụ|Ừ|Ứ|Ử|Ữ|Ự|Ỳ|Ý|Ỷ|Ỹ|Ỵ',\n",
    "        lambda x: dicchar[x.group()], txt)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "    Start section: Chuyển câu văn về kiểu gõ telex khi không bật Unikey\n",
    "    Ví dụ: thủy = thuyr, tượng = tuwowngj\n",
    "\"\"\"\n",
    "bang_nguyen_am = [['a', 'à', 'á', 'ả', 'ã', 'ạ', 'a'],\n",
    "                  ['ă', 'ằ', 'ắ', 'ẳ', 'ẵ', 'ặ', 'aw'],\n",
    "                  ['â', 'ầ', 'ấ', 'ẩ', 'ẫ', 'ậ', 'aa'],\n",
    "                  ['e', 'è', 'é', 'ẻ', 'ẽ', 'ẹ', 'e'],\n",
    "                  ['ê', 'ề', 'ế', 'ể', 'ễ', 'ệ', 'ee'],\n",
    "                  ['i', 'ì', 'í', 'ỉ', 'ĩ', 'ị', 'i'],\n",
    "                  ['o', 'ò', 'ó', 'ỏ', 'õ', 'ọ', 'o'],\n",
    "                  ['ô', 'ồ', 'ố', 'ổ', 'ỗ', 'ộ', 'oo'],\n",
    "                  ['ơ', 'ờ', 'ớ', 'ở', 'ỡ', 'ợ', 'ow'],\n",
    "                  ['u', 'ù', 'ú', 'ủ', 'ũ', 'ụ', 'u'],\n",
    "                  ['ư', 'ừ', 'ứ', 'ử', 'ữ', 'ự', 'uw'],\n",
    "                  ['y', 'ỳ', 'ý', 'ỷ', 'ỹ', 'ỵ', 'y']]\n",
    "bang_ky_tu_dau = ['', 'f', 's', 'r', 'x', 'j']\n",
    "\n",
    "nguyen_am_to_ids = {}\n",
    "\n",
    "for i in range(len(bang_nguyen_am)):\n",
    "    for j in range(len(bang_nguyen_am[i]) - 1):\n",
    "        nguyen_am_to_ids[bang_nguyen_am[i][j]] = (i, j)\n",
    "\n",
    "\n",
    "def vn_word_to_telex_type(word):\n",
    "    dau_cau = 0\n",
    "    new_word = ''\n",
    "    for char in word:\n",
    "        x, y = nguyen_am_to_ids.get(char, (-1, -1))\n",
    "        if x == -1:\n",
    "            new_word += char\n",
    "            continue\n",
    "        if y != 0:\n",
    "            dau_cau = y\n",
    "        new_word += bang_nguyen_am[x][-1]\n",
    "    new_word += bang_ky_tu_dau[dau_cau]\n",
    "    return new_word\n",
    "\n",
    "\n",
    "def vn_sentence_to_telex_type(sentence):\n",
    "    \"\"\"\n",
    "    Chuyển câu tiếng việt có dấu về kiểu gõ telex.\n",
    "    :param sentence:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    words = sentence.split()\n",
    "    for index, word in enumerate(words):\n",
    "        words[index] = vn_word_to_telex_type(word)\n",
    "    return ' '.join(words)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "    End section: Chuyển câu văn về kiểu gõ telex khi không bật Unikey\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "    Start section: Chuyển câu văn về cách gõ dấu kiểu cũ: dùng òa úy thay oà uý\n",
    "    Xem tại đây: https://vi.wikipedia.org/wiki/Quy_t%E1%BA%AFc_%C4%91%E1%BA%B7t_d%E1%BA%A5u_thanh_trong_ch%E1%BB%AF_qu%E1%BB%91c_ng%E1%BB%AF\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def vietnameseWordNormalizer(word):\n",
    "    if not is_valid_vietnam_word(word):\n",
    "        return word\n",
    "\n",
    "    chars = list(word)\n",
    "    dau_cau = 0\n",
    "    nguyen_am_index = []\n",
    "    qu_or_gi = False\n",
    "    for index, char in enumerate(chars):\n",
    "        x, y = nguyen_am_to_ids.get(char, (-1, -1))\n",
    "        if x == -1:\n",
    "            continue\n",
    "        elif x == 9:  # check qu\n",
    "            if index != 0 and chars[index - 1] == 'q':\n",
    "                chars[index] = 'u'\n",
    "                qu_or_gi = True\n",
    "        elif x == 5:  # check gi\n",
    "            if index != 0 and chars[index - 1] == 'g':\n",
    "                chars[index] = 'i'\n",
    "                qu_or_gi = True\n",
    "        if y != 0:\n",
    "            dau_cau = y\n",
    "            chars[index] = bang_nguyen_am[x][0]\n",
    "        if not qu_or_gi or index != 1:\n",
    "            nguyen_am_index.append(index)\n",
    "    if len(nguyen_am_index) < 2:\n",
    "        if qu_or_gi:\n",
    "            if len(chars) == 2:\n",
    "                x, y = nguyen_am_to_ids.get(chars[1])\n",
    "                chars[1] = bang_nguyen_am[x][dau_cau]\n",
    "            else:\n",
    "                x, y = nguyen_am_to_ids.get(chars[2], (-1, -1))\n",
    "                if x != -1:\n",
    "                    chars[2] = bang_nguyen_am[x][dau_cau]\n",
    "                else:\n",
    "                    chars[1] = bang_nguyen_am[5][dau_cau] if chars[1] == 'i' else bang_nguyen_am[9][dau_cau]\n",
    "            return ''.join(chars)\n",
    "        return word\n",
    "\n",
    "    for index in nguyen_am_index:\n",
    "        x, y = nguyen_am_to_ids[chars[index]]\n",
    "        if x == 4 or x == 8:  # ê, ơ\n",
    "            chars[index] = bang_nguyen_am[x][dau_cau]\n",
    "            # for index2 in nguyen_am_index:\n",
    "            #     if index2 != index:\n",
    "            #         x, y = nguyen_am_to_ids[chars[index]]\n",
    "            #         chars[index2] = bang_nguyen_am[x][0]\n",
    "            return ''.join(chars)\n",
    "\n",
    "    if len(nguyen_am_index) == 2:\n",
    "        if nguyen_am_index[-1] == len(chars) - 1:\n",
    "            x, y = nguyen_am_to_ids[chars[nguyen_am_index[0]]]\n",
    "            chars[nguyen_am_index[0]] = bang_nguyen_am[x][dau_cau]\n",
    "            # x, y = nguyen_am_to_ids[chars[nguyen_am_index[1]]]\n",
    "            # chars[nguyen_am_index[1]] = bang_nguyen_am[x][0]\n",
    "        else:\n",
    "            # x, y = nguyen_am_to_ids[chars[nguyen_am_index[0]]]\n",
    "            # chars[nguyen_am_index[0]] = bang_nguyen_am[x][0]\n",
    "            x, y = nguyen_am_to_ids[chars[nguyen_am_index[1]]]\n",
    "            chars[nguyen_am_index[1]] = bang_nguyen_am[x][dau_cau]\n",
    "    else:\n",
    "        # x, y = nguyen_am_to_ids[chars[nguyen_am_index[0]]]\n",
    "        # chars[nguyen_am_index[0]] = bang_nguyen_am[x][0]\n",
    "        x, y = nguyen_am_to_ids[chars[nguyen_am_index[1]]]\n",
    "        chars[nguyen_am_index[1]] = bang_nguyen_am[x][dau_cau]\n",
    "        # x, y = nguyen_am_to_ids[chars[nguyen_am_index[2]]]\n",
    "        # chars[nguyen_am_index[2]] = bang_nguyen_am[x][0]\n",
    "    return ''.join(chars)\n",
    "\n",
    "\n",
    "def is_valid_vietnam_word(word):\n",
    "    chars = list(word)\n",
    "    nguyen_am_index = -1\n",
    "    for index, char in enumerate(chars):\n",
    "        x, y = nguyen_am_to_ids.get(char, (-1, -1))\n",
    "        if x != -1:\n",
    "            if nguyen_am_index == -1:\n",
    "                nguyen_am_index = index\n",
    "            else:\n",
    "                if index - nguyen_am_index != 1:\n",
    "                    return False\n",
    "                nguyen_am_index = index\n",
    "    return True\n",
    "\n",
    "\n",
    "def vietnameseTextNormalizer(sentence):\n",
    "    \"\"\"\n",
    "        Chuyển câu tiếng việt về chuẩn gõ dấu kiểu cũ.\n",
    "        :param sentence:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "    sentence = sentence.lower()\n",
    "    words = sentence.split()\n",
    "    for index, word in enumerate(words):\n",
    "        cw = re.sub(r'(^\\p{P}*)([p{L}.]*\\p{L}+)(\\p{P}*$)', r'\\1/\\2/\\3', word).split('/')\n",
    "        # print(cw)\n",
    "        if len(cw) == 3:\n",
    "            cw[1] = vietnameseWordNormalizer(cw[1])\n",
    "        words[index] = ''.join(cw)\n",
    "    return ' '.join(words)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "    End section: Chuyển câu văn về cách gõ dấu kiểu cũ: dùng òa úy thay oà uý\n",
    "    Xem tại đây: https://vi.wikipedia.org/wiki/Quy_tắc_đặt_dấu_thanh_trong_chữ_quốc_ngữ\n",
    "\"\"\"\n",
    "if __name__ == '__main__':\n",
    "    print(vietnameseTextNormalizer('anh hoà, đang làm.. gì'))\n",
    "    # f = open('/home/lap60313/data/corpus-full.txt', encoding='utf8')\n",
    "    # sentence = f.readline()\n",
    "    # current_line = 0\n",
    "    # while sentence:\n",
    "    #     current_line += 1\n",
    "    #     if current_line % 1000 == 0:\n",
    "    #         print('Current line', str(current_line))\n",
    "    #     sentence = sentence.lower().strip()\n",
    "    #     sentence = convertwindown1525toutf8(sentence)\n",
    "    #     sentence = vietnameseTextNormalizer(sentence)\n",
    "    #     with open('/home/lap60313/data/corpus-full.txt.out', 'a+', encoding='utf8') as fp:\n",
    "    #         fp.write(sentence + \"\\n\")\n",
    "    #     sentence = f.readline()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tách từ\n",
    "\n",
    "File từ điển các từ và cụm từ được tổng hợp từ các nguồn sau:\n",
    "- Viet74K.txt: https://github.com/undertheseanlp/underthesea/tree/main/underthesea/corpus/data\n",
    "- words.txt: https://github.com/undertheseanlp/underthesea/tree/main/datasets/DI_Vietnamese-UVD/corpus/dictionary\n",
    "- vi-vocab: https://github.com/vncorenlp/VnCoreNLP/tree/master/models/wordsegmenter\n",
    "- Thư mục Words - Danh mục từ của wordnet: https://github.com/zeloru/vietnamese-wordnet/tree/master\n",
    "\n",
    "Sau đó được xử lý để tạo thành 1 file từ điển duy nhất."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Số lượng từ ghép và cụm từ trong vocab: 69335\n",
      "Số bộ vocab phân theo độ dài: 19\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'nhưng sự thực_hiện vẫn còn chưa phù_hợp'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "import unicodedata as ud\n",
    "import re\n",
    "\n",
    "def syllablize(sentence): # Tách âm tiết cho một câu tiếng Việt\n",
    "    word = '\\w+'\n",
    "    non_word = '[^\\w\\s]'\n",
    "    digits = '\\d+([\\.,_]\\d+)+'\n",
    "    \n",
    "    patterns = []\n",
    "    patterns.extend([word, non_word, digits])\n",
    "    patterns = f\"({'|'.join(patterns)})\"\n",
    "    \n",
    "    sentence = ud.normalize('NFC', sentence)\n",
    "    tokens = re.findall(patterns, sentence, re.UNICODE)\n",
    "    return [token[0] for token in tokens]\n",
    "\n",
    "# Tải từ trong vi-vocab.txt\n",
    "with open('data/dic.txt', encoding='utf8') as f:\n",
    "    vocab = f.read().split('\\n')\n",
    "# Xây dựng từ điển vocabs theo độ dài từ\n",
    "vocabs = defaultdict(list)\n",
    "for word in vocab:\n",
    "    vocabs[len(word.split())].append(word)\n",
    "\n",
    "print('Số lượng từ ghép và cụm từ trong vocab:', len(vocab))\n",
    "print('Số bộ vocab phân theo độ dài:', len(vocabs))\n",
    "\n",
    "def longest_matching(sentence, vocabs):\n",
    "  words = syllablize(sentence) # tách âm tiết cho câu\n",
    "  result = []\n",
    "  i = len(words)-1 # index của từ hiện tại\n",
    "  while i > -1: \n",
    "    word = '' \n",
    "    # tìm kiếm trong từ điển theo chiều dài của từ ưu tiên từ dài trước\n",
    "    for j in range(i+1):\n",
    "      ls_word = words[j:i+1]\n",
    "      word = ' '.join(ls_word)\n",
    "      # xem thử có trong từ điển không\n",
    "      if word.lower() in vocabs.get(len(ls_word), []):\n",
    "        i = j\n",
    "        break\n",
    "    result = [word] + result\n",
    "    i-=1\n",
    "  return result # return the final list\n",
    "\n",
    "def tokenize_sentences(sentence):\n",
    "    return ' '.join([x.replace(' ','_') for x in longest_matching(sentence, vocabs)])\n",
    "\n",
    "tokenize_sentences('nhưng sự thực hiện vẫn còn chưa phù hợp')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hoàn thiện tiền xử lý"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_preprocess(document):\n",
    "    # xóa html code\n",
    "    document = remove_html(document)\n",
    "    # chuẩn hóa unicode\n",
    "    document = convert_unicode(document)\n",
    "    # chuẩn hóa cách gõ dấu tiếng Việt\n",
    "    document = vietnameseTextNormalizer(document)\n",
    "    # tách từ\n",
    "    document = tokenize_sentences(document)\n",
    "    # đưa về lower\n",
    "    document = document.lower()\n",
    "    # xóa các ký tự không cần thiết\n",
    "    document = re.sub(r'[^\\s\\wáàảãạăắằẳẵặâấầẩẫậéèẻẽẹêếềểễệóòỏõọôốồổỗộơớờởỡợíìỉĩịúùủũụưứừửữựýỳỷỹỵđ_]',' ',document)\n",
    "    # xóa khoảng trắng thừa\n",
    "    document = re.sub(r'\\s+', ' ', document).strip()\n",
    "    return document"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loại bỏ stopword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/stopwords.txt', encoding='utf8') as f:\n",
    "    stopword = f.read().replace(' ','_').split('\\n')\n",
    "\n",
    "stopword = set(stopword)\n",
    "\n",
    "def remove_stopwords(line):\n",
    "    words = []\n",
    "    for word in line.strip().split():\n",
    "        if word not in stopword:\n",
    "            words.append(word)\n",
    "    return ' '.join(words)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Xây dựng mô hình phân loại văn bản"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Xây dựng tập dữ liệu huấn luyện và kiểm thử"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Xây dựng mô hình phân loại"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Đánh giá mô hình phân loại văn bản"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tài liệu tham khảo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "OpenCV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
