{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bài toán\n",
    "\n",
    "https://blog.luyencode.net/phan-loai-van-ban-tieng-viet/#bai-toan-phan-loai-van-ban\n",
    "\n",
    "- Input: một đoạn văn bản\n",
    "- Output: một trong các nhãn sau: 'thể thao', 'giáo dục', 'giải trí', 'kinh doanh', 'pháp luật', 'sức khỏe', 'số hóa', 'thế giới', 'thời sự', 'xe'\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thu thập dữ liệu"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Dữ liệu được lấy dừ https://github.com/duyvuleo/VNTC/tree/master/Data/10Topics/Ver1.1 với thông tin như sau:\n",
    "\n",
    "***Train***\n",
    "|Topic |\tTopic ID |\t#files |\n",
    "|--|--|--|\n",
    "| Chinh tri Xa hoi\t| XH |\t5219 |\n",
    "| Doi song\t| DS |\t\t3159 |\n",
    "| Khoa hoc\t| KH |\t\t1820 |\n",
    "| Kinh doanh\t| KD |\t\t2552 |\n",
    "| Phap luat\t| PL |\t\t3868 |\n",
    "| Suc khoe\t| SK |\t\t3384 |\n",
    "| The gioi\t| TG |\t\t2898 |\n",
    "| The thao\t| TT |\t\t5298 |\n",
    "| Van hoa \t| VH |\t\t3080 |\n",
    "| Vi tinh\t\t| VT |\t\t2481 |\n",
    "\n",
    "Total\t\t\t\t33759\n",
    "\n",
    "***Test***\n",
    "|Topic |\tTopic ID |\t#files |\n",
    "|--|--|--|\n",
    "| Chinh tri Xa hoi\t| XH |\t7567 |\n",
    "| Doi song\t| DS |\t\t2036 |\n",
    "| Khoa hoc\t| KH |\t\t2096 |\n",
    "| Kinh doanh\t| KD |\t\t5276 |\n",
    "| Phap luat\t| PL |\t\t3788 |\n",
    "| Suc khoe\t| SK |\t\t5417 |\n",
    "| The gioi\t| TG |\t\t6716 |\n",
    "| The thao\t| TT |\t\t6667 |\n",
    "| Van hoa\t\t| VH |\t\t6250 |\n",
    "| Vi tinh\t\t| VT |\t\t4560 |\n",
    "\n",
    "Total\t\t\t\t50373\n",
    "\n",
    "Tên file được đặt cào từ:\n",
    "\n",
    "+ DS_VNE_(...) : VnExpress news agency (http://vnexpress.net/)\n",
    "+ DS_TT_(...):  Youth news agency (http://tuoitre.vn/)\n",
    "+ DS_TN_(...): Thanh Nien news agency (http://thanhnien.vn/)\n",
    "+ DS_NLD_(...): Nguoi Lao Dong news agency (http://nld.com.vn/)\n",
    "\n",
    "File zip chứa toàn bộ file, tên mỗi file là Nhãn_Báo_ (STT).txt VD: XH_NLD_ (3675).txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "\n",
    "def extract_data_from_zip(zip_file_path):\n",
    "    result = []\n",
    "    with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "        file_list = zip_ref.namelist()  # Lấy danh sách tên các file trong zip\n",
    "\n",
    "        for file_name in file_list:\n",
    "            if file_name.endswith('.txt'):  # Chỉ xử lý các file có đuôi .txt\n",
    "                with zip_ref.open(file_name) as file:\n",
    "                    data = file.read().decode('utf-16-le')  # Nó được mã hoá bằng utf-16-le\n",
    "                    # nội dung file có dạng \\ufeff<content>, đôi khi có dấu cách ở đầu và cuối\n",
    "                    content = data[1:].strip()\n",
    "                    label = file_name.split('/')[2].split('_')[:2]\n",
    "                    result.append((content, label))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33759"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = extract_data_from_zip('data/Train_Full.zip')\n",
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50373"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = extract_data_from_zip('data/Test_Full.zip')\n",
    "len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'DS', 'KD', 'KH', 'PL', 'SK', 'TG', 'TT', 'VH', 'VT', 'XH'}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = [x[0] for _, x in train_data]\n",
    "labels = set(labels)\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The gioi, Khoa hoc, The thao, Kinh doanh, Suc khoe, Chinh tri Xa hoi, Van hoa, Phap luat, Doi song, Vi tinh\n"
     ]
    }
   ],
   "source": [
    "label_dict={'XH':'Chinh tri Xa hoi', \n",
    "            'DS':'Doi song', \n",
    "            'KH':'Khoa hoc', \n",
    "            'KD':'Kinh doanh', \n",
    "            'PL':'Phap luat', \n",
    "            'SK':'Suc khoe', \n",
    "            'TG':'The gioi', \n",
    "            'TT':'The thao', \n",
    "            'VH':'Van hoa', \n",
    "            'VT':'Vi tinh', }\n",
    "print(*[label_dict[x] for x in labels], sep = ', ')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tiền xử lý dữ liệu"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Xoá HTML\n",
    "\n",
    "Dữ liệu được thu thập từ các website đôi khi vẫn còn sót lại các đoạn mã HTML. Các mã HTML code này là rác, chẳng những không có tác dụng cho việc phân loại mà còn làm kết quả phân loại văn bản bị kém đi. Do đó, cần phải loại bỏ các đoạn mã HTML này."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' This is an example nè'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "def remove_html(txt):\n",
    "    return re.sub(r'<[^>]*>', ' ', txt)\n",
    "\n",
    "txt = \"<p class=\\\"par\\\">This is an example</p>nè\"\n",
    "remove_html(txt)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chuẩn hoá Tiếng Việt\n",
    "\n",
    "- **Chuẩn hoá Unicode**: Hiện nay, có 2 loại mã Unicode được sử dụng phổ biến, Unicode tổ hợp và Unicode dựng sẵn. Hướng xử lý: Đưa về 1 chuẩn Unicode dựng sẵn (thằng này phổ biến hơn)\n",
    "- **Chuẩn hoán cách bỏ dấu**: Chuyển câu văn về cách gõ dấu kiểu cũ: dùng òa úy thay oà uý"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False False False True\n",
      "True True True True\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "uniChars = \"àáảãạâầấẩẫậăằắẳẵặèéẻẽẹêềếểễệđìíỉĩịòóỏõọôồốổỗộơờớởỡợùúủũụưừứửữựỳýỷỹỵÀÁẢÃẠÂẦẤẨẪẬĂẰẮẲẴẶÈÉẺẼẸÊỀẾỂỄỆĐÌÍỈĨỊÒÓỎÕỌÔỒỐỔỖỘƠỜỚỞỠỢÙÚỦŨỤƯỪỨỬỮỰỲÝỶỸỴÂĂĐÔƠƯ\"\n",
    "unsignChars = \"aaaaaaaaaaaaaaaaaeeeeeeeeeeediiiiiooooooooooooooooouuuuuuuuuuuyyyyyAAAAAAAAAAAAAAAAAEEEEEEEEEEEDIIIOOOOOOOOOOOOOOOOOOOUUUUUUUUUUUYYYYYAADOOU\"\n",
    "\n",
    "\n",
    "def loaddicchar():\n",
    "    dic = {}\n",
    "    char1252 = 'à|á|ả|ã|ạ|ầ|ấ|ẩ|ẫ|ậ|ằ|ắ|ẳ|ẵ|ặ|è|é|ẻ|ẽ|ẹ|ề|ế|ể|ễ|ệ|ì|í|ỉ|ĩ|ị|ò|ó|ỏ|õ|ọ|ồ|ố|ổ|ỗ|ộ|ờ|ớ|ở|ỡ|ợ|ù|ú|ủ|ũ|ụ|ừ|ứ|ử|ữ|ự|ỳ|ý|ỷ|ỹ|ỵ|À|Á|Ả|Ã|Ạ|Ầ|Ấ|Ẩ|Ẫ|Ậ|Ằ|Ắ|Ẳ|Ẵ|Ặ|È|É|Ẻ|Ẽ|Ẹ|Ề|Ế|Ể|Ễ|Ệ|Ì|Í|Ỉ|Ĩ|Ị|Ò|Ó|Ỏ|Õ|Ọ|Ồ|Ố|Ổ|Ỗ|Ộ|Ờ|Ớ|Ở|Ỡ|Ợ|Ù|Ú|Ủ|Ũ|Ụ|Ừ|Ứ|Ử|Ữ|Ự|Ỳ|Ý|Ỷ|Ỹ|Ỵ'.split(\n",
    "        '|')\n",
    "    charutf8 = \"à|á|ả|ã|ạ|ầ|ấ|ẩ|ẫ|ậ|ằ|ắ|ẳ|ẵ|ặ|è|é|ẻ|ẽ|ẹ|ề|ế|ể|ễ|ệ|ì|í|ỉ|ĩ|ị|ò|ó|ỏ|õ|ọ|ồ|ố|ổ|ỗ|ộ|ờ|ớ|ở|ỡ|ợ|ù|ú|ủ|ũ|ụ|ừ|ứ|ử|ữ|ự|ỳ|ý|ỷ|ỹ|ỵ|À|Á|Ả|Ã|Ạ|Ầ|Ấ|Ẩ|Ẫ|Ậ|Ằ|Ắ|Ẳ|Ẵ|Ặ|È|É|Ẻ|Ẽ|Ẹ|Ề|Ế|Ể|Ễ|Ệ|Ì|Í|Ỉ|Ĩ|Ị|Ò|Ó|Ỏ|Õ|Ọ|Ồ|Ố|Ổ|Ỗ|Ộ|Ờ|Ớ|Ở|Ỡ|Ợ|Ù|Ú|Ủ|Ũ|Ụ|Ừ|Ứ|Ử|Ữ|Ự|Ỳ|Ý|Ỷ|Ỹ|Ỵ\".split(\n",
    "        '|')\n",
    "    for i in range(len(char1252)):\n",
    "        dic[char1252[i]] = charutf8[i]\n",
    "    return dic\n",
    "\n",
    "\n",
    "dicchar = loaddicchar()\n",
    "\n",
    "\n",
    "def convert_unicode(txt):\n",
    "    return re.sub(\n",
    "        r'à|á|ả|ã|ạ|ầ|ấ|ẩ|ẫ|ậ|ằ|ắ|ẳ|ẵ|ặ|è|é|ẻ|ẽ|ẹ|ề|ế|ể|ễ|ệ|ì|í|ỉ|ĩ|ị|ò|ó|ỏ|õ|ọ|ồ|ố|ổ|ỗ|ộ|ờ|ớ|ở|ỡ|ợ|ù|ú|ủ|ũ|ụ|ừ|ứ|ử|ữ|ự|ỳ|ý|ỷ|ỹ|ỵ|À|Á|Ả|Ã|Ạ|Ầ|Ấ|Ẩ|Ẫ|Ậ|Ằ|Ắ|Ẳ|Ẵ|Ặ|È|É|Ẻ|Ẽ|Ẹ|Ề|Ế|Ể|Ễ|Ệ|Ì|Í|Ỉ|Ĩ|Ị|Ò|Ó|Ỏ|Õ|Ọ|Ồ|Ố|Ổ|Ỗ|Ộ|Ờ|Ớ|Ở|Ỡ|Ợ|Ù|Ú|Ủ|Ũ|Ụ|Ừ|Ứ|Ử|Ữ|Ự|Ỳ|Ý|Ỷ|Ỹ|Ỵ',\n",
    "        lambda x: dicchar[x.group()], txt)\n",
    "    \n",
    "dict_map = {\n",
    "    \"òa\": \"oà\",\n",
    "    \"Òa\": \"Oà\",\n",
    "    \"ÒA\": \"OÀ\",\n",
    "    \"óa\": \"oá\",\n",
    "    \"Óa\": \"Oá\",\n",
    "    \"ÓA\": \"OÁ\",\n",
    "    \"ỏa\": \"oả\",\n",
    "    \"Ỏa\": \"Oả\",\n",
    "    \"ỎA\": \"OẢ\",\n",
    "    \"õa\": \"oã\",\n",
    "    \"Õa\": \"Oã\",\n",
    "    \"ÕA\": \"OÃ\",\n",
    "    \"ọa\": \"oạ\",\n",
    "    \"Ọa\": \"Oạ\",\n",
    "    \"ỌA\": \"OẠ\",\n",
    "    \"òe\": \"oè\",\n",
    "    \"Òe\": \"Oè\",\n",
    "    \"ÒE\": \"OÈ\",\n",
    "    \"óe\": \"oé\",\n",
    "    \"Óe\": \"Oé\",\n",
    "    \"ÓE\": \"OÉ\",\n",
    "    \"ỏe\": \"oẻ\",\n",
    "    \"Ỏe\": \"Oẻ\",\n",
    "    \"ỎE\": \"OẺ\",\n",
    "    \"õe\": \"oẽ\",\n",
    "    \"Õe\": \"Oẽ\",\n",
    "    \"ÕE\": \"OẼ\",\n",
    "    \"ọe\": \"oẹ\",\n",
    "    \"Ọe\": \"Oẹ\",\n",
    "    \"ỌE\": \"OẸ\",\n",
    "    \"ùy\": \"uỳ\",\n",
    "    \"Ùy\": \"Uỳ\",\n",
    "    \"ÙY\": \"UỲ\",\n",
    "    \"úy\": \"uý\",\n",
    "    \"Úy\": \"Uý\",\n",
    "    \"ÚY\": \"UÝ\",\n",
    "    \"ủy\": \"uỷ\",\n",
    "    \"Ủy\": \"Uỷ\",\n",
    "    \"ỦY\": \"UỶ\",\n",
    "    \"ũy\": \"uỹ\",\n",
    "    \"Ũy\": \"Uỹ\",\n",
    "    \"ŨY\": \"UỸ\",\n",
    "    \"ụy\": \"uỵ\",\n",
    "    \"Ụy\": \"Uỵ\",\n",
    "    \"ỤY\": \"UỴ\",\n",
    "    }\n",
    "\n",
    "def replace_all(text, dict_map):\n",
    "    for i, j in dict_map.items():\n",
    "        text = text.replace(i, j)\n",
    "    return text\n",
    "def vietnameseTextNormalizer(sentence):\n",
    "    import unicodedata    \n",
    "    return replace_all(unicodedata.normalize('NFC', convert_unicode(sentence)), dict_map)\n",
    "\n",
    "str_utf8 = 'Anh Hòa, đang làm gì chị Thúy vậy, ăn qụyt phải không?' # Unicode (dựng sẵn - dấu theo ký tự)\n",
    "str_utf8_2 = 'Anh Hoà, đang làm gì chị Thuý vậy, ăn quỵt phải không?' # Unicode (dựng sẵn - dấu theo ký tự)\n",
    "str_com = 'Anh Hòa, đang làm gì chị Thúy vậy, ăn qụyt phải không?'  # Unicode composite (tổ hợp - dấu riêng)\n",
    "str_1252 = 'Anh Hòa, đang làm gì chị Thúy vậy, ăn qụyt phải không?' # Windows-1252 = Latin-1\n",
    "print(str_utf8 == str_utf8_2, str_utf8 == str_com, str_utf8 == str_1252, str_com == str_1252)\n",
    "str_utf8 = vietnameseTextNormalizer(str_utf8)\n",
    "str_utf8 = vietnameseTextNormalizer(str_utf8_2)\n",
    "str_com = vietnameseTextNormalizer(str_com)\n",
    "str_1252 = vietnameseTextNormalizer(str_1252)\n",
    "print(str_utf8 == str_utf8_2, str_utf8 == str_com, str_utf8 == str_1252, str_com == str_1252)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tách từ\n",
    "\n",
    "File từ điển các từ và cụm từ được tổng hợp từ các nguồn sau:\n",
    "- Viet74K.txt: https://github.com/undertheseanlp/underthesea/tree/main/underthesea/corpus/data\n",
    "- words.txt: https://github.com/undertheseanlp/underthesea/tree/main/datasets/DI_Vietnamese-UVD/corpus/dictionary\n",
    "- vi-vocab: https://github.com/vncorenlp/VnCoreNLP/tree/master/models/wordsegmenter\n",
    "- Thư mục Words - Danh mục từ của wordnet: https://github.com/zeloru/vietnamese-wordnet/tree/master\n",
    "\n",
    "Sau đó được xử lý để tạo thành 1 file từ điển duy nhất tên là [dic3.txt](data/dic3.txt) nhờ code từ: [Create_Data.ipynb](Create_Data.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Số lượng từ ghép và cụm từ trong vocab: 112343\n",
      "Số bộ vocab phân theo độ dài: 20\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'nhưng sự_thực_hiện vẫn_còn chưa phù_hợp'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "import re\n",
    "\n",
    "def syllablize(sentence): # Tách âm tiết cho một câu tiếng Việt\n",
    "    word = '\\w+'\n",
    "    non_word = '[^\\w\\s]'\n",
    "    digits = '\\d+([\\.,_]\\d+)+'\n",
    "    \n",
    "    patterns = []\n",
    "    patterns.extend([word, non_word, digits])\n",
    "    patterns = f\"({'|'.join(patterns)})\"\n",
    "    \n",
    "    tokens = re.findall(patterns, sentence, re.UNICODE)\n",
    "    return [token[0] for token in tokens]\n",
    "\n",
    "# Tải từ trong vi-vocab.txt\n",
    "with open('data/dic3.txt', encoding='utf8') as f:\n",
    "    vocab = f.read().split('\\n')\n",
    "# Xây dựng từ điển vocabs theo độ dài từ\n",
    "vocabs = defaultdict(list)\n",
    "for word in vocab:\n",
    "    vocabs[len(word.split())].append(word)\n",
    "\n",
    "print('Số lượng từ ghép và cụm từ trong vocab:', len(vocab))\n",
    "print('Số bộ vocab phân theo độ dài:', len(vocabs))\n",
    "\n",
    "def longest_matching(sentence, vocabs):\n",
    "  words = syllablize(sentence) # tách âm tiết cho câu\n",
    "  result = []\n",
    "  i = len(words)-1 # index của từ hiện tại\n",
    "  while i > -1: \n",
    "    word = '' \n",
    "    # tìm kiếm trong từ điển theo chiều dài của từ ưu tiên từ dài trước\n",
    "    for j in range(i+1):\n",
    "      ls_word = words[j:i+1]\n",
    "      word = ' '.join(ls_word)\n",
    "      # xem thử có trong từ điển không\n",
    "      if word.lower() in vocabs.get(len(ls_word), []):\n",
    "        i = j\n",
    "        break\n",
    "    result = [word] + result\n",
    "    i-=1\n",
    "  return result # return the final list\n",
    "\n",
    "def tokenize_sentences(sentence):\n",
    "    return ' '.join([x.replace(' ','_') for x in longest_matching(sentence, vocabs)])\n",
    "\n",
    "tokenize_sentences('nhưng sự thực hiện vẫn còn chưa phù hợp')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hoàn thiện tiền xử lý"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'têu đề bài_báo có_vẻ_như anh hoà đang làm_gì đó với chị thuý vậy có_thể_là ăn_quỵt có_phải không'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def text_preprocess(document):\n",
    "    # xóa html code\n",
    "    document = remove_html(document)\n",
    "    # chuẩn hóa unicode\n",
    "    document = convert_unicode(document)\n",
    "    # chuẩn hóa cách gõ dấu tiếng Việt\n",
    "    document = vietnameseTextNormalizer(document)\n",
    "    # tách từ\n",
    "    document = tokenize_sentences(document)\n",
    "    # đưa về lower\n",
    "    document = document.lower()\n",
    "    # xóa các ký tự không cần thiết\n",
    "    document = re.sub(r'[^\\s\\wáàảãạăắằẳẵặâấầẩẫậéèẻẽẹêếềểễệóòỏõọôốồổỗộơớờởỡợíìỉĩịúùủũụưứừửữựýỳỷỹỵđ_]',' ',document)\n",
    "    # xóa khoảng trắng thừa\n",
    "    document = re.sub(r'\\s+', ' ', document).strip()\n",
    "    return document\n",
    "\n",
    "text = text_preprocess('<p class=\\\"par\\\">Têu đề bài báo:</p>Có vẻ như Anh Hòa, đang làm gì đó với chị Thúy vậy, có thể là ăn qụyt có phải không?')\n",
    "text"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loại bỏ stopword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'têu đề bài_báo có_vẻ_như hoà thuý có_thể_là ăn_quỵt'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('data/stopwords-nlp-vi.txt', encoding='utf8') as f:\n",
    "    stopword = f.read().replace(' ','_').split('\\n')\n",
    "\n",
    "stopword = set(stopword)\n",
    "\n",
    "def remove_stopwords(line):\n",
    "    words = []\n",
    "    for word in line.strip().split():\n",
    "        if word not in stopword:\n",
    "            words.append(word)\n",
    "    return ' '.join(words)\n",
    "\n",
    "text = remove_stopwords(text)\n",
    "text"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Xây dựng mô hình phân loại văn bản"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch is using GPU.\n",
      "Current GPU device: 0\n",
      "Number of available GPUs: 1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"PyTorch is using GPU.\")\n",
    "    device = torch.device(\"cuda\")  # Chọn GPU làm thiết bị tính toán\n",
    "    print(f\"Current GPU device: {torch.cuda.current_device()}\")\n",
    "    print(f\"Number of available GPUs: {torch.cuda.device_count()}\")\n",
    "else:\n",
    "    print(\"PyTorch is using CPU.\")\n",
    "    device = torch.device(\"cpu\")  # Chọn CPU làthiết bị tính toán\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trong mô hình phân loại văn bản thì `vocab_size`, `embedding_dim`, `hidden_size`, và `num_classes` là các thông số quan trọng cần được xác định trước khi xây dựng mô hình:\n",
    "\n",
    "1. `vocab_size` là kích thước của từ điển (vocabulary) trong mô hình phân loại văn bản. Nó đại diện cho số lượng từ duy nhất có trong tập dữ liệu huấn luyện. Khi xây dựng mô hình, mỗi từ sẽ được biểu diễn bằng một chỉ số số nguyên từ 0 đến `vocab_size - 1`. `vocab_size` có thể được tính toán tự động bằng cách đếm từ.\n",
    "\n",
    "Để tính toán `vocab_size`, bạn có thể xây dựng từ điển từ các từ trong tập dữ liệu huấn luyện và đếm số lượng từ duy nhất. Một cách đơn giản để làm điều này là sử dụng một bộ đếm từ (word counter) để đếm số lần xuất hiện của mỗi từ trong tập dữ liệu và sau đó lấy độ dài của từ điển để có `vocab_size`.\n",
    "\n",
    "2. `embedding_dim`: Đây là số chiều của không gian nhúng (embedding space) trong mô hình. Trong quá trình huấn luyện, các từ trong từ điển sẽ được biểu diễn bằng các vectơ có kích thước `embedding_dim`. Số chiều này cần được chọn sao cho đủ lớn để mô hình có thể học được các đặc trưng quan trọng của văn bản, nhưng cũng không quá lớn để tránh tăng quá nhiều tham số và tốn thời gian huấn luyện.Thông thường, kích thước không gian nhúng từ 100 đến 300 chiều đã được sử dụng hiệu quả trong nhiều nhiệm vụ phân loại văn bản.\n",
    "\n",
    "3. `hidden_size`: Đây là số lượng đơn vị ẩn trong mạng LSTM (Long Short-Term Memory) hoặc các mạng RNN (Recurrent Neural Network) khác. `hidden_size` ảnh hưởng đến khả năng mô hình học các mẫu dữ liệu phức tạp. Nếu `hidden_size` lớn, mô hình có khả năng học các mẫu phức tạp hơn, nhưng đồng thời tăng cường độ phức tạp của mô hình và thời gian huấn luyện. Giá trị thông thường cho `hidden_size` trong mạng LSTM hoặc RNN là 100, 200 hoặc 300. Tuy nhiên, nếu tập dữ liệu lớn hoặc bài toán phân loại phức tạp hơn, có thể cần tăng giá trị này để mô hình có khả năng học mẫu phức tạp hơn.\n",
    "\n",
    "4. `num_classes`: Đây là số lượng lớp trong bài toán phân loại. Đối với bài toán phân loại văn bản, `num_classes` sẽ là số lượng nhãn khác nhau mà chúng ta muốn mô hình phân loại các văn bản vào. Ví dụ, nếu ta có 3 nhãn: \"positive\", \"negative\", và \"neutral\", thì `num_classes` sẽ là 3.\n",
    "\n",
    "Việc lựa chọn các giá trị tối ưu cho `embedding_dim`, `hidden_size`, và `num_classes` phụ thuộc vào bài toán cụ thể và dữ liệu đang làm việc. Thông thường, các giá trị này được chọn dựa trên kinh nghiệm thực tế và thử nghiệm. Trong quá trình huấn luyện, bạn có thể điều chỉnh các giá trị này và theo dõi hiệu suất của mô hình để tìm ra các giá trị phù hợp nhất."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lớp tập dữ liệu tùy chỉnh\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        text, label = self.data[index]\n",
    "        preprocessed_text = text_preprocess(text)\n",
    "        processed_text = remove_stopwords(preprocessed_text)\n",
    "        return processed_text, label\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "from collections import Counter\n",
    "\n",
    "# Tính toán từ điển và vocab_size\n",
    "word_counter = Counter()\n",
    "for text, _ in train_data:\n",
    "    preprocessed_text = text_preprocess(text)\n",
    "    processed_text = remove_stopwords(preprocessed_text)\n",
    "    word_counter.update(processed_text.split())\n",
    "\n",
    "vocab_size = len(word_counter)\n",
    "embedding_dim = 100\n",
    "hidden_size = 128\n",
    "num_classes = len(labels)\n",
    "\n",
    "# Mô hình phân loại văn bản\n",
    "class TextClassifier(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(TextClassifier, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        output, _ = self.lstm(embedded)\n",
    "        output = output[:, -1, :]\n",
    "        logits = self.fc(output)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cấu hình\n",
    "batch_size = 32\n",
    "epochs = 10\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Chuẩn bị dữ liệu\n",
    "train_dataset = CustomDataset(train_data)\n",
    "test_dataset = CustomDataset(test_data)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Khởi tạo mô hình và tối ưu hóa\n",
    "model = TextClassifier(num_classes).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Huấn luyện\n",
    "model.train()\n",
    "for epoch in range(epochs):\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Đánh giá mô hình\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    accuracy = 100 * correct / total\n",
    "    print(f\"Accuracy: {accuracy}%\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Xây dựng tập dữ liệu huấn luyện và kiểm thử"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Xây dựng mô hình phân loại"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Đánh giá mô hình phân loại văn bản"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tài liệu tham khảo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "OpenCV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
