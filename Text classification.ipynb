{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bài toán\n",
    "\n",
    "Các khó khăn gặp phải khi xây dựng mô hình phân loại văn bản tiếng Việt:\n",
    "\n",
    "1. Việc tách từ tiếng Việt khá là phức tạp do có nhiều từ và cụm từ ghép lại với nhau, nhiều từ không có trong từ điển.\n",
    "2. Việc tìm được bộ từ điển tương đối đầy đủ khá khó khăn do tiếng Việt có nhiều từ ngữ cả trong lĩnh vực chuyên ngành và nhiều từ ngữ không có trong từ điển. Hiện tại mới chỉ tìm được một số ít từ phổ biến có trong các từ điển mở.\n",
    "3. Việc triển khai mô hình phân loại văn bản tiếng Việt khá là khó khăn do không có nhiều tài liệu tham khảo, đặc biệt là các tài liệu về xử lý ngôn ngữ tự nhiên tiếng Việt.\n",
    "4. Việc cài đặt môi trường lập trình cho Tensorflow cũng như pytorch gặp nhiều khó khăn khi cài đặt trên Windows, đặc biệt là khi sử dụng GPU.\n",
    "5. Việc chuẩn hoá văn bản tiếng Việt gặp rất nhiều thách thức về:\n",
    "    - Cách mã hoá văn bản Unicode\n",
    "    - Cách bỏ dấu tiếng Việt\n",
    "    - Xử lý sai chính tả\n",
    "    - Xử lý từ viết tắt\n",
    "    - Xử lý các từ ngữ không có trong từ điển\n",
    "7. Việc tách từ cũng gặp rất nhiều bất lợi\n",
    "    - Từ điển càng lớn thì việc tách từ càng chậm, đã thử nghiệm và tách từ mất 30p cho tập train.\n",
    "    - Tokenization phải đào tạo mô hình để thực hiện mới đem lại hiệu quả tốt. Đã thử nghiệm với các thư viện như pyvi, underthesea, vncorenlp, ... nhưng kết quả không tốt. Chậm quá\n",
    "6. Hiện chưa có bộ StopWords tiếng Việt tương đối đầy đủ, nên việc xử lý StopWords cũng gặp nhiều khó khăn do đa phần là tự tạo không theo một tiêu chuẩn nào cả.\n",
    "\n",
    "- Input: một đoạn văn bản\n",
    "- Output: một trong các nhãn sau: 'XH':'Chinh tri Xa hoi', \n",
    "            'DS':'Doi song', \n",
    "            'KH':'Khoa hoc', \n",
    "            'KD':'Kinh doanh', \n",
    "            'PL':'Phap luat', \n",
    "            'SK':'Suc khoe', \n",
    "            'TG':'The gioi', \n",
    "            'TT':'The thao', \n",
    "            'VH':'Van hoa', \n",
    "            'VT':'Vi tinh',\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thu thập dữ liệu"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Dữ liệu được lấy dừ https://github.com/duyvuleo/VNTC/tree/master/Data/10Topics/Ver1.1 với thông tin như sau:\n",
    "\n",
    "***Train***\n",
    "|Topic |\tTopic ID |\t#files |\n",
    "|--|--|--|\n",
    "| Chinh tri Xa hoi\t| XH |\t5219 |\n",
    "| Doi song\t| DS |\t\t3159 |\n",
    "| Khoa hoc\t| KH |\t\t1820 |\n",
    "| Kinh doanh\t| KD |\t\t2552 |\n",
    "| Phap luat\t| PL |\t\t3868 |\n",
    "| Suc khoe\t| SK |\t\t3384 |\n",
    "| The gioi\t| TG |\t\t2898 |\n",
    "| The thao\t| TT |\t\t5298 |\n",
    "| Van hoa \t| VH |\t\t3080 |\n",
    "| Vi tinh\t\t| VT |\t\t2481 |\n",
    "\n",
    "Total\t\t\t\t33759\n",
    "\n",
    "***Test***\n",
    "|Topic |\tTopic ID |\t#files |\n",
    "|--|--|--|\n",
    "| Chinh tri Xa hoi\t| XH |\t7567 |\n",
    "| Doi song\t| DS |\t\t2036 |\n",
    "| Khoa hoc\t| KH |\t\t2096 |\n",
    "| Kinh doanh\t| KD |\t\t5276 |\n",
    "| Phap luat\t| PL |\t\t3788 |\n",
    "| Suc khoe\t| SK |\t\t5417 |\n",
    "| The gioi\t| TG |\t\t6716 |\n",
    "| The thao\t| TT |\t\t6667 |\n",
    "| Van hoa\t\t| VH |\t\t6250 |\n",
    "| Vi tinh\t\t| VT |\t\t4560 |\n",
    "\n",
    "Total\t\t\t\t50373\n",
    "\n",
    "Tên file được đặt cào từ:\n",
    "\n",
    "+ DS_VNE_(...) : VnExpress news agency (http://vnexpress.net/)\n",
    "+ DS_TT_(...):  Youth news agency (http://tuoitre.vn/)\n",
    "+ DS_TN_(...): Thanh Nien news agency (http://thanhnien.vn/)\n",
    "+ DS_NLD_(...): Nguoi Lao Dong news agency (http://nld.com.vn/)\n",
    "\n",
    "File zip chứa toàn bộ file, tên mỗi file là Nhãn_Báo_ (STT).txt VD: XH_NLD_ (3675).txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "\n",
    "def extract_data_from_zip(zip_file_path, length = 50000):\n",
    "    result = []\n",
    "    with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "        file_list = zip_ref.namelist()  # Lấy danh sách tên các file trong zip\n",
    "\n",
    "        for file_name in file_list:\n",
    "            if file_name.endswith('.txt'):  # Chỉ xử lý các file có đuôi .txt\n",
    "                with zip_ref.open(file_name) as file:\n",
    "                    data = file.read().decode('utf-16-le')  # Nó được mã hoá bằng utf-16-le\n",
    "                    # nội dung file có dạng \\ufeff<content>, đôi khi có dấu cách ở đầu và cuối\n",
    "                    content = data[1:].strip()\n",
    "                    words = content.split(' ')\n",
    "                    content = ' '.join(words[:min(length, len(words))])\n",
    "                    label = file_name.split('/')[2].split('_')[:2]\n",
    "                    result.append([content, label])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33759"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = extract_data_from_zip('data/Train_Full.zip')\n",
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50373"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = extract_data_from_zip('data/Test_Full.zip')\n",
    "len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'DS', 'KD', 'KH', 'PL', 'SK', 'TG', 'TT', 'VH', 'VT', 'XH'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = [x[0] for _, x in train_data]\n",
    "labels = set(labels)\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phap luat, Suc khoe, Khoa hoc, Vi tinh, Kinh doanh, The gioi, The thao, Van hoa, Chinh tri Xa hoi, Doi song\n"
     ]
    }
   ],
   "source": [
    "label_dict={'XH':'Chinh tri Xa hoi', \n",
    "            'DS':'Doi song', \n",
    "            'KH':'Khoa hoc', \n",
    "            'KD':'Kinh doanh', \n",
    "            'PL':'Phap luat', \n",
    "            'SK':'Suc khoe', \n",
    "            'TG':'The gioi', \n",
    "            'TT':'The thao', \n",
    "            'VH':'Van hoa', \n",
    "            'VT':'Vi tinh', }\n",
    "print(*[label_dict[x] for x in labels], sep = ', ')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tiền xử lý dữ liệu"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Xoá HTML\n",
    "\n",
    "Dữ liệu được thu thập từ các website đôi khi vẫn còn sót lại các đoạn mã HTML. Các mã HTML code này là rác, chẳng những không có tác dụng cho việc phân loại mà còn làm kết quả phân loại văn bản bị kém đi. Do đó, cần phải loại bỏ các đoạn mã HTML này."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' This is an example nè'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "def remove_html(txt):\n",
    "    return re.sub(r'<[^>]*>', ' ', txt)\n",
    "\n",
    "txt = \"<p class=\\\"par\\\">This is an example</p>nè\"\n",
    "remove_html(txt)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chuẩn hoá Tiếng Việt\n",
    "\n",
    "- **Chuẩn hoá Unicode**: Hiện nay, có 2 loại mã Unicode được sử dụng phổ biến, Unicode tổ hợp và Unicode dựng sẵn. Hướng xử lý: Đưa về 1 chuẩn Unicode dựng sẵn (thằng này phổ biến hơn)\n",
    "- **Chuẩn hoán cách bỏ dấu**: Chuyển câu văn về cách gõ dấu kiểu cũ: dùng òa úy thay oà uý"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False False False True\n",
      "True True True True\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "uniChars = \"àáảãạâầấẩẫậăằắẳẵặèéẻẽẹêềếểễệđìíỉĩịòóỏõọôồốổỗộơờớởỡợùúủũụưừứửữựỳýỷỹỵÀÁẢÃẠÂẦẤẨẪẬĂẰẮẲẴẶÈÉẺẼẸÊỀẾỂỄỆĐÌÍỈĨỊÒÓỎÕỌÔỒỐỔỖỘƠỜỚỞỠỢÙÚỦŨỤƯỪỨỬỮỰỲÝỶỸỴÂĂĐÔƠƯ\"\n",
    "unsignChars = \"aaaaaaaaaaaaaaaaaeeeeeeeeeeediiiiiooooooooooooooooouuuuuuuuuuuyyyyyAAAAAAAAAAAAAAAAAEEEEEEEEEEEDIIIOOOOOOOOOOOOOOOOOOOUUUUUUUUUUUYYYYYAADOOU\"\n",
    "\n",
    "\n",
    "def loaddicchar():\n",
    "    dic = {}\n",
    "    char1252 = 'à|á|ả|ã|ạ|ầ|ấ|ẩ|ẫ|ậ|ằ|ắ|ẳ|ẵ|ặ|è|é|ẻ|ẽ|ẹ|ề|ế|ể|ễ|ệ|ì|í|ỉ|ĩ|ị|ò|ó|ỏ|õ|ọ|ồ|ố|ổ|ỗ|ộ|ờ|ớ|ở|ỡ|ợ|ù|ú|ủ|ũ|ụ|ừ|ứ|ử|ữ|ự|ỳ|ý|ỷ|ỹ|ỵ|À|Á|Ả|Ã|Ạ|Ầ|Ấ|Ẩ|Ẫ|Ậ|Ằ|Ắ|Ẳ|Ẵ|Ặ|È|É|Ẻ|Ẽ|Ẹ|Ề|Ế|Ể|Ễ|Ệ|Ì|Í|Ỉ|Ĩ|Ị|Ò|Ó|Ỏ|Õ|Ọ|Ồ|Ố|Ổ|Ỗ|Ộ|Ờ|Ớ|Ở|Ỡ|Ợ|Ù|Ú|Ủ|Ũ|Ụ|Ừ|Ứ|Ử|Ữ|Ự|Ỳ|Ý|Ỷ|Ỹ|Ỵ'.split(\n",
    "        '|')\n",
    "    charutf8 = \"à|á|ả|ã|ạ|ầ|ấ|ẩ|ẫ|ậ|ằ|ắ|ẳ|ẵ|ặ|è|é|ẻ|ẽ|ẹ|ề|ế|ể|ễ|ệ|ì|í|ỉ|ĩ|ị|ò|ó|ỏ|õ|ọ|ồ|ố|ổ|ỗ|ộ|ờ|ớ|ở|ỡ|ợ|ù|ú|ủ|ũ|ụ|ừ|ứ|ử|ữ|ự|ỳ|ý|ỷ|ỹ|ỵ|À|Á|Ả|Ã|Ạ|Ầ|Ấ|Ẩ|Ẫ|Ậ|Ằ|Ắ|Ẳ|Ẵ|Ặ|È|É|Ẻ|Ẽ|Ẹ|Ề|Ế|Ể|Ễ|Ệ|Ì|Í|Ỉ|Ĩ|Ị|Ò|Ó|Ỏ|Õ|Ọ|Ồ|Ố|Ổ|Ỗ|Ộ|Ờ|Ớ|Ở|Ỡ|Ợ|Ù|Ú|Ủ|Ũ|Ụ|Ừ|Ứ|Ử|Ữ|Ự|Ỳ|Ý|Ỷ|Ỹ|Ỵ\".split(\n",
    "        '|')\n",
    "    for i in range(len(char1252)):\n",
    "        dic[char1252[i]] = charutf8[i]\n",
    "    return dic\n",
    "\n",
    "dicchar = loaddicchar()\n",
    "\n",
    "def convert_unicode(txt):\n",
    "    return re.sub(\n",
    "        r'à|á|ả|ã|ạ|ầ|ấ|ẩ|ẫ|ậ|ằ|ắ|ẳ|ẵ|ặ|è|é|ẻ|ẽ|ẹ|ề|ế|ể|ễ|ệ|ì|í|ỉ|ĩ|ị|ò|ó|ỏ|õ|ọ|ồ|ố|ổ|ỗ|ộ|ờ|ớ|ở|ỡ|ợ|ù|ú|ủ|ũ|ụ|ừ|ứ|ử|ữ|ự|ỳ|ý|ỷ|ỹ|ỵ|À|Á|Ả|Ã|Ạ|Ầ|Ấ|Ẩ|Ẫ|Ậ|Ằ|Ắ|Ẳ|Ẵ|Ặ|È|É|Ẻ|Ẽ|Ẹ|Ề|Ế|Ể|Ễ|Ệ|Ì|Í|Ỉ|Ĩ|Ị|Ò|Ó|Ỏ|Õ|Ọ|Ồ|Ố|Ổ|Ỗ|Ộ|Ờ|Ớ|Ở|Ỡ|Ợ|Ù|Ú|Ủ|Ũ|Ụ|Ừ|Ứ|Ử|Ữ|Ự|Ỳ|Ý|Ỷ|Ỹ|Ỵ',\n",
    "        lambda x: dicchar[x.group()], txt)\n",
    "    \n",
    "dict_map = {\"òa\": \"oà\", \"Òa\": \"Oà\", \"ÒA\": \"OÀ\", \"óa\": \"oá\", \"Óa\": \"Oá\", \"ÓA\": \"OÁ\", \"ỏa\": \"oả\", \"Ỏa\": \"Oả\", \"ỎA\": \"OẢ\", \"õa\": \"oã\", \"Õa\": \"Oã\", \"ÕA\": \"OÃ\", \"ọa\": \"oạ\", \"Ọa\": \"Oạ\", \"ỌA\": \"OẠ\", \"òe\": \"oè\", \"Òe\": \"Oè\", \"ÒE\": \"OÈ\", \"óe\": \"oé\", \"Óe\": \"Oé\", \"ÓE\": \"OÉ\", \"ỏe\": \"oẻ\", \"Ỏe\": \"Oẻ\", \"ỎE\": \"OẺ\", \"õe\": \"oẽ\", \"Õe\": \"Oẽ\", \"ÕE\": \"OẼ\", \"ọe\": \"oẹ\", \"Ọe\": \"Oẹ\", \"ỌE\": \"OẸ\", \"ùy\": \"uỳ\", \"Ùy\": \"Uỳ\", \"ÙY\": \"UỲ\", \"úy\": \"uý\", \"Úy\": \"Uý\", \"ÚY\": \"UÝ\", \"ủy\": \"uỷ\", \"Ủy\": \"Uỷ\", \"ỦY\": \"UỶ\", \"ũy\": \"uỹ\", \"Ũy\": \"Uỹ\", \"ŨY\": \"UỸ\", \"ụy\": \"uỵ\", \"Ụy\": \"Uỵ\", \"ỤY\": \"UỴ\",}\n",
    "\n",
    "def replace_all(text, dict_map):\n",
    "    for i, j in dict_map.items():\n",
    "        text = text.replace(i, j)\n",
    "    return text\n",
    "def vietnameseTextNormalizer(sentence):\n",
    "    import unicodedata    \n",
    "    return replace_all(unicodedata.normalize('NFC', convert_unicode(sentence)), dict_map)\n",
    "\n",
    "str_utf8 = 'Anh Hòa, đang làm gì chị Thúy vậy, ăn qụyt phải không?' # Unicode (dựng sẵn - dấu theo ký tự)\n",
    "str_utf8_2 = 'Anh Hoà, đang làm gì chị Thuý vậy, ăn quỵt phải không?' # Unicode (dựng sẵn - dấu theo ký tự)\n",
    "str_com = 'Anh Hòa, đang làm gì chị Thúy vậy, ăn qụyt phải không?'  # Unicode composite (tổ hợp - dấu riêng)\n",
    "str_1252 = 'Anh Hòa, đang làm gì chị Thúy vậy, ăn qụyt phải không?' # Windows-1252 = Latin-1\n",
    "print(str_utf8 == str_utf8_2, str_utf8 == str_com, str_utf8 == str_1252, str_com == str_1252)\n",
    "str_utf8 = vietnameseTextNormalizer(str_utf8)\n",
    "str_utf8 = vietnameseTextNormalizer(str_utf8_2)\n",
    "str_com = vietnameseTextNormalizer(str_com)\n",
    "str_1252 = vietnameseTextNormalizer(str_1252)\n",
    "print(str_utf8 == str_utf8_2, str_utf8 == str_com, str_utf8 == str_1252, str_com == str_1252)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tách từ\n",
    "\n",
    "File từ điển các từ và cụm từ được tổng hợp từ các nguồn sau:\n",
    "- Viet74K.txt: https://github.com/undertheseanlp/underthesea/tree/main/underthesea/corpus/data\n",
    "- words.txt: https://github.com/undertheseanlp/underthesea/tree/main/datasets/DI_Vietnamese-UVD/corpus/dictionary\n",
    "- vi-vocab: https://github.com/vncorenlp/VnCoreNLP/tree/master/models/wordsegmenter\n",
    "- Thư mục Words - Danh mục từ của wordnet: https://github.com/zeloru/vietnamese-wordnet/tree/master\n",
    "\n",
    "Sau đó được xử lý để tạo thành 1 file từ điển duy nhất tên là [dic3.txt](data/dic3.txt) nhờ code từ: [Create_Data.ipynb](Create_Data.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Số lượng từ ghép và cụm từ trong vocab: 112343\n",
      "Số bộ vocab phân theo độ dài: 20\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'nhưng sự_thực_hiện vẫn_còn chưa phù_hợp'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "import re\n",
    "\n",
    "def syllablize(sentence): # Tách âm tiết cho một câu tiếng Việt\n",
    "    word = '\\w+'\n",
    "    non_word = '[^\\w\\s]'\n",
    "    digits = '\\d+([\\.,_]\\d+)+'\n",
    "    \n",
    "    patterns = []\n",
    "    patterns.extend([word, non_word, digits])\n",
    "    patterns = f\"({'|'.join(patterns)})\"\n",
    "    \n",
    "    tokens = re.findall(patterns, sentence, re.UNICODE)\n",
    "    return [token[0] for token in tokens]\n",
    "\n",
    "# Tải từ trong vi-vocab.txt\n",
    "with open('data/dic3.txt', encoding='utf8') as f:\n",
    "    vocab = f.read().split('\\n')\n",
    "# Xây dựng từ điển vocabs theo độ dài từ\n",
    "vocabs = defaultdict(dict) # dùng list chậm hơn so với dict khoảng 25 lần\n",
    "for word in vocab:\n",
    "    vocabs[len(word.split())][word.lower()] = True\n",
    "\n",
    "print('Số lượng từ ghép và cụm từ trong vocab:', len(vocab))\n",
    "print('Số bộ vocab phân theo độ dài:', len(vocabs))\n",
    "\n",
    "def longest_matching(sentence, vocabs):\n",
    "  words = syllablize(sentence) # tách âm tiết cho câu\n",
    "  result = []\n",
    "  i = len(words)-1 # index của từ hiện tại\n",
    "  while i > -1: \n",
    "    word = '' \n",
    "    # tìm kiếm trong từ điển theo chiều dài của từ ưu tiên từ dài trước\n",
    "    for j in range(max(i-8,0), i+1):\n",
    "      ls_word = words[j:i+1]\n",
    "      word = ' '.join(ls_word)\n",
    "      # xem thử có trong từ điển không\n",
    "      if word.lower() in vocabs.get(i-j+1, []):\n",
    "        i = j\n",
    "        break\n",
    "    result.append(word)\n",
    "    i-=1\n",
    "  return result[::-1] # return the final list\n",
    "\n",
    "def tokenize_sentences(sentence):\n",
    "    return ' '.join([x.replace(' ','_') for x in longest_matching(sentence, vocabs)])\n",
    "\n",
    "tokenize_sentences('nhưng sự thực hiện vẫn còn chưa phù hợp')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hoàn thiện tiền xử lý"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'têu đề bài_báo có_vẻ_như anh hoà đang làm_gì đó với chị thuý vậy có_thể_là ăn_quỵt có_phải không'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def text_preprocess(document):\n",
    "    # xóa html code\n",
    "    document = remove_html(document)\n",
    "    # chuẩn hóa unicode\n",
    "    document = convert_unicode(document)\n",
    "    # chuẩn hóa cách gõ dấu tiếng Việt\n",
    "    document = vietnameseTextNormalizer(document)\n",
    "    # tách từ\n",
    "    document = tokenize_sentences(document)\n",
    "    # đưa về lower\n",
    "    document = document.lower()\n",
    "    # xóa các ký tự không cần thiết\n",
    "    document = re.sub(r'[^\\s\\wáàảãạăắằẳẵặâấầẩẫậéèẻẽẹêếềểễệóòỏõọôốồổỗộơớờởỡợíìỉĩịúùủũụưứừửữựýỳỷỹỵđ_]',' ',document)\n",
    "    # xóa khoảng trắng thừa\n",
    "    document = re.sub(r'\\s+', ' ', document).strip()\n",
    "    return document\n",
    "\n",
    "text = text_preprocess('<p class=\\\"par\\\">Têu đề bài báo:</p>Có vẻ như Anh Hòa, đang làm gì đó với chị Thúy vậy, có thể là ăn qụyt có phải không?')\n",
    "text"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loại bỏ stopword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'têu đề bài_báo có_vẻ_như hoà thuý có_thể_là ăn_quỵt'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('data/stopwords-nlp-vi.txt', encoding='utf8') as f:\n",
    "    stopword = f.read().replace(' ','_').split('\\n')\n",
    "\n",
    "stopword = set(stopword)\n",
    "\n",
    "def remove_stopwords(line):\n",
    "    words = []\n",
    "    for word in line.strip().split():\n",
    "        if word not in stopword:\n",
    "            words.append(word)\n",
    "    return ' '.join(words)\n",
    "\n",
    "text = remove_stopwords(text)\n",
    "text"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Xây dựng mô hình phân loại văn bản"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch is using GPU.\n",
      "Current GPU device: 0\n",
      "Number of available GPUs: 1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"PyTorch is using GPU.\")\n",
    "    device = torch.device(\"cuda\")  # Chọn GPU làm thiết bị tính toán\n",
    "    print(f\"Current GPU device: {torch.cuda.current_device()}\")\n",
    "    print(f\"Number of available GPUs: {torch.cuda.device_count()}\")\n",
    "else:\n",
    "    print(\"PyTorch is using CPU.\")\n",
    "    device = torch.device(\"cpu\")  # Chọn CPU là thiết bị tính toán\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trong mô hình phân loại văn bản thì `vocab_size`, `embedding_dim`, `hidden_size`, và `num_classes` là các thông số quan trọng cần được xác định trước khi xây dựng mô hình:\n",
    "\n",
    "1. `vocab_size` là kích thước của từ điển (vocabulary) trong mô hình phân loại văn bản. Nó đại diện cho số lượng từ duy nhất có trong tập dữ liệu huấn luyện. Khi xây dựng mô hình, mỗi từ sẽ được biểu diễn bằng một chỉ số số nguyên từ 0 đến `vocab_size - 1`. `vocab_size` có thể được tính toán tự động bằng cách đếm từ.\n",
    "\n",
    "Để tính toán `vocab_size`, bạn có thể xây dựng từ điển từ các từ trong tập dữ liệu huấn luyện và đếm số lượng từ duy nhất. Một cách đơn giản để làm điều này là sử dụng một bộ đếm từ (word counter) để đếm số lần xuất hiện của mỗi từ trong tập dữ liệu và sau đó lấy độ dài của từ điển để có `vocab_size`.\n",
    "\n",
    "2. `embedding_dim`: Đây là số chiều của không gian nhúng (embedding space) trong mô hình. Trong quá trình huấn luyện, các từ trong từ điển sẽ được biểu diễn bằng các vectơ có kích thước `embedding_dim`. Số chiều này cần được chọn sao cho đủ lớn để mô hình có thể học được các đặc trưng quan trọng của văn bản, nhưng cũng không quá lớn để tránh tăng quá nhiều tham số và tốn thời gian huấn luyện.Thông thường, kích thước không gian nhúng từ 100 đến 300 chiều đã được sử dụng hiệu quả trong nhiều nhiệm vụ phân loại văn bản.\n",
    "\n",
    "3. `hidden_size`: Đây là số lượng đơn vị ẩn trong mạng LSTM (Long Short-Term Memory) hoặc các mạng RNN (Recurrent Neural Network) khác. `hidden_size` ảnh hưởng đến khả năng mô hình học các mẫu dữ liệu phức tạp. Nếu `hidden_size` lớn, mô hình có khả năng học các mẫu phức tạp hơn, nhưng đồng thời tăng cường độ phức tạp của mô hình và thời gian huấn luyện. Giá trị thông thường cho `hidden_size` trong mạng LSTM hoặc RNN là 100, 200 hoặc 300. Tuy nhiên, nếu tập dữ liệu lớn hoặc bài toán phân loại phức tạp hơn, có thể cần tăng giá trị này để mô hình có khả năng học mẫu phức tạp hơn.\n",
    "\n",
    "4. `num_classes`: Đây là số lượng lớp trong bài toán phân loại. Đối với bài toán phân loại văn bản, `num_classes` sẽ là số lượng nhãn khác nhau mà chúng ta muốn mô hình phân loại các văn bản vào. Ví dụ, nếu ta có 3 nhãn: \"positive\", \"negative\", và \"neutral\", thì `num_classes` sẽ là 3.\n",
    "\n",
    "Việc lựa chọn các giá trị tối ưu cho `embedding_dim`, `hidden_size`, và `num_classes` phụ thuộc vào bài toán cụ thể và dữ liệu đang làm việc. Thông thường, các giá trị này được chọn dựa trên kinh nghiệm thực tế và thử nghiệm. Trong quá trình huấn luyện, bạn có thể điều chỉnh các giá trị này và theo dõi hiệu suất của mô hình để tìm ra các giá trị phù hợp nhất."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPreprocessed_text(text):\n",
    "    text = text_preprocess(text)\n",
    "    text = remove_stopwords(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lớp tập dữ liệu tùy chỉnh\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        text, label = self.data[index]\n",
    "        return torch.tensor(getPreprocessed_text(text)), label\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 1686/33759 [00:14<04:40, 114.17it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[53], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m tqdm(\u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(train_data))):\n\u001b[0;32m      8\u001b[0m     text, _ \u001b[39m=\u001b[39m train_data[i]\n\u001b[1;32m----> 9\u001b[0m     word_counter\u001b[39m.\u001b[39mupdate(getPreprocessed_text(text)\u001b[39m.\u001b[39msplit())\n\u001b[0;32m     11\u001b[0m vocab_size \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(word_counter)\n\u001b[0;32m     12\u001b[0m embedding_dim \u001b[39m=\u001b[39m \u001b[39m100\u001b[39m\n",
      "Cell \u001b[1;32mIn[40], line 2\u001b[0m, in \u001b[0;36mgetPreprocessed_text\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgetPreprocessed_text\u001b[39m(text):\n\u001b[1;32m----> 2\u001b[0m     text \u001b[39m=\u001b[39m text_preprocess(text)\n\u001b[0;32m      3\u001b[0m     text \u001b[39m=\u001b[39m remove_stopwords(text)\n\u001b[0;32m      4\u001b[0m     \u001b[39mreturn\u001b[39;00m text\n",
      "Cell \u001b[1;32mIn[9], line 9\u001b[0m, in \u001b[0;36mtext_preprocess\u001b[1;34m(document)\u001b[0m\n\u001b[0;32m      7\u001b[0m document \u001b[39m=\u001b[39m vietnameseTextNormalizer(document)\n\u001b[0;32m      8\u001b[0m \u001b[39m# tách từ\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m document \u001b[39m=\u001b[39m tokenize_sentences(document)\n\u001b[0;32m     10\u001b[0m \u001b[39m# đưa về lower\u001b[39;00m\n\u001b[0;32m     11\u001b[0m document \u001b[39m=\u001b[39m document\u001b[39m.\u001b[39mlower()\n",
      "Cell \u001b[1;32mIn[52], line 46\u001b[0m, in \u001b[0;36mtokenize_sentences\u001b[1;34m(sentence)\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtokenize_sentences\u001b[39m(sentence):\n\u001b[1;32m---> 46\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin([x\u001b[39m.\u001b[39mreplace(\u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39m_\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m longest_matching(sentence, vocabs)])\n",
      "Cell \u001b[1;32mIn[52], line 28\u001b[0m, in \u001b[0;36mlongest_matching\u001b[1;34m(sentence, vocabs)\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mlongest_matching\u001b[39m(sentence, vocabs):\n\u001b[1;32m---> 28\u001b[0m   words \u001b[39m=\u001b[39m syllablize(sentence) \u001b[39m# tách âm tiết cho câu\u001b[39;00m\n\u001b[0;32m     29\u001b[0m   result \u001b[39m=\u001b[39m []\n\u001b[0;32m     30\u001b[0m   i \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(words)\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m \u001b[39m# index của từ hiện tại\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[52], line 13\u001b[0m, in \u001b[0;36msyllablize\u001b[1;34m(sentence)\u001b[0m\n\u001b[0;32m     10\u001b[0m patterns\u001b[39m.\u001b[39mextend([word, non_word, digits])\n\u001b[0;32m     11\u001b[0m patterns \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m(\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m|\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(patterns)\u001b[39m}\u001b[39;00m\u001b[39m)\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m---> 13\u001b[0m tokens \u001b[39m=\u001b[39m re\u001b[39m.\u001b[39;49mfindall(patterns, sentence, re\u001b[39m.\u001b[39;49mUNICODE)\n\u001b[0;32m     14\u001b[0m \u001b[39mreturn\u001b[39;00m [token[\u001b[39m0\u001b[39m] \u001b[39mfor\u001b[39;00m token \u001b[39min\u001b[39;00m tokens]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Tính toán từ điển và vocab_size\n",
    "word_counter = Counter()\n",
    "\n",
    "for i in tqdm(range(len(train_data))):\n",
    "    text, _ = train_data[i]\n",
    "    word_counter.update(getPreprocessed_text(text).split())\n",
    "\n",
    "vocab_size = len(word_counter)\n",
    "embedding_dim = 100\n",
    "hidden_size = 128\n",
    "num_classes = len(labels)\n",
    "\n",
    "print(vocab_size, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mô hình phân loại văn bản\n",
    "class TextClassifier(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(TextClassifier, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        output, _ = self.lstm(embedded)\n",
    "        output = output[:, -1, :]\n",
    "        logits = self.fc(output)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cấu hình\n",
    "batch_size = 32\n",
    "epochs = 10\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Chuẩn bị dữ liệu\n",
    "train_dataset = CustomDataset(train_data)\n",
    "test_dataset = CustomDataset(test_data)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Khởi tạo mô hình và tối ưu hóa\n",
    "model = TextClassifier(num_classes).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "new(): invalid data type 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m model\u001b[39m.\u001b[39mtrain()\n\u001b[0;32m      3\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m tqdm(\u001b[39mrange\u001b[39m(epochs)):\n\u001b[1;32m----> 4\u001b[0m     \u001b[39mfor\u001b[39;00m inputs, labels \u001b[39min\u001b[39;00m train_loader:\n\u001b[0;32m      5\u001b[0m         \u001b[39mprint\u001b[39m(inputs)\n\u001b[0;32m      6\u001b[0m         \u001b[39mprint\u001b[39m(labels)\n",
      "File \u001b[1;32md:\\SOFTS\\Anacoda\\envs\\Tensor\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    631\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    632\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 633\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    634\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    635\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    636\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    637\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32md:\\SOFTS\\Anacoda\\envs\\Tensor\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:677\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    675\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    676\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 677\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    678\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[0;32m    679\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32md:\\SOFTS\\Anacoda\\envs\\Tensor\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32md:\\SOFTS\\Anacoda\\envs\\Tensor\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[1;32mIn[13], line 8\u001b[0m, in \u001b[0;36mCustomDataset.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getitem__\u001b[39m(\u001b[39mself\u001b[39m, index):\n\u001b[0;32m      7\u001b[0m     text, label \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata[index]\n\u001b[1;32m----> 8\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mtensor(getPreprocessed_text(text)), label\n",
      "\u001b[1;31mTypeError\u001b[0m: new(): invalid data type 'str'"
     ]
    }
   ],
   "source": [
    "# Huấn luyện\n",
    "model.train()\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    for inputs, labels in train_loader:\n",
    "        print(inputs)\n",
    "        print(labels)\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Đánh giá mô hình\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    accuracy = 100 * correct / total\n",
    "    print(f\"Accuracy: {accuracy}%\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Xây dựng tập dữ liệu huấn luyện và kiểm thử"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Xây dựng mô hình phân loại"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Đánh giá mô hình phân loại văn bản"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tài liệu tham khảo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "OpenCV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
